## 分布式锁
以下摘抄了一些网上关于分布式锁实现的讨论，后面有我自己的一些思考。

我们需要的分布式锁应该是怎么样的？（这里以方法锁为例，资源锁同理）

+ 可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。

+ 这把锁要是一把可重入锁（避免死锁）

+ 这把锁最好是一把阻塞锁（根据业务需求考虑要不要这条）

+ 有高可用的获取锁和释放锁功能

+ 获取锁和释放锁的性能要好

### 一 基于 `ZooKeeper`
***如果使用基于`ZooKeeper`的分布式锁，推荐`Curator`客户端，这个客户端中封装了一个可重入的锁服务。***
#### 1.1 实现原理
基于 `ZooKeeper` 瞬时有序节点实现的分布式锁，大致思想即为：每个客户端对某个功能加锁时，在 `ZooKeeper` 上的与该功能对应的指定节点的目录下，生成一个唯一的瞬时有序节点。判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。

+ 客户端尝试创建一个`znode`节点，比如`/lock`。那么第一个客户端就创建成功了，相当于拿到了锁；而其它的客户端会创建失败（`znode`已存在），获取锁失败。
+ 持有锁的客户端访问共享资源完成后，将 `znode` 删掉，这样其它客户端接下来就能来获取锁了。
+ `znode` 应该被创建成 `ephemeral` 的（临时节点）。这是 `znode` 的一个特性，它保证如果创建 `znode` 的那个客户端崩溃了，那么相应的 `znode` 会被自动删除。这保证了锁一定会被释放。

来看下`Zookeeper`能不能解决前面提到的问题。

+ 锁无法释放？

使用`Zookeeper`可以有效的解决锁无法释放的问题，因为在创建锁的时候，客户端会在`ZK`中创建一个临时节点，一旦客户端获取到锁之后突然挂掉（`Session`连接断开），那么这个临时节点就会自动删除掉。其他客户端就可以再次获得锁。

+ 非阻塞锁？

使用`Zookeeper`可以实现阻塞的锁，客户端可以通过在ZK中创建顺序节点，并且在节点上绑定监听器，一旦节点有变化，`Zookeeper`会通知客户端，客户端可以检查自己创建的节点是不是当前所有节点中序号最小的，如果是，那么自己就获取到锁，便可以执行业务逻辑了。

+ 不可重入？

使用`Zookeeper`也可以有效的解决不可重入的问题，客户端在创建节点的时候，把当前客户端的主机信息和线程信息直接写入到节点中，下次想要获取锁的时候和当前最小的节点中的数据比对一下就可以了。如果和自己的信息一样，那么自己直接获取到锁，如果不一样就再创建一个临时的顺序节点，参与排队。

+ 单点问题？

使用`Zookeeper`可以有效的解决单点问题，`ZK`是集群部署的，只要集群中有半数以上的机器存活，就可以对外提供服务。

可以直接使用`zookeeper`第三方库`Curator`客户端，这个客户端中封装了一个可重入的锁服务。`Curator`提供的`InterProcessMutex`是分布式锁的实现。`acquire`方法用户获取锁，`release`方法用于释放锁。

注意：Curator 的版本问题

The are currently two released versions of Curator, 2.x.x and 3.x.x:

+ Curator 2.x.x - compatible with both ZooKeeper 3.4.x and ZooKeeper 3.5.x

+ Curator 3.x.x - compatible only with ZooKeeper 3.5.x and includes support for new features such as dynamic reconfiguration, etc.


使用ZK实现的分布式锁好像完全符合了本文开头我们对一个分布式锁的所有期望。但是，其实并不是，`Zookeeper`实现的分布式锁其实存在一个缺点，那就是性能上可能并没有缓存服务那么高。因为每次在创建锁和释放锁的过程中，都要动态创建、销毁瞬时节点来实现锁功能。`ZK`中创建和删除节点只能通过`Leader`服务器来执行，然后将数据同步到所有的`Follower`机器上。

用`ZooKeeper`实现的分布式锁也不一定就是安全的。`ZooKeeper`是怎么检测出某个客户端已经崩溃了呢？实际上，每个客户端都与`ZooKeeper`的某台服务器维护着一个`Session`，这个`Session`依赖定期的心跳（`heartbeat`）来维持。如果`ZooKeeper`长时间收不到客户端的心跳（这个时间称为`Session`的过期时间），那么它就认为`Session`过期了，通过这个`Session`所创建的所有的`ephemeral`类型的`znode`节点都会被自动删除。

#### 1.2 官方推荐

Fully distributed locks that are globally synchronous, meaning at any snapshot in time no two clients think they hold the same lock. These can be implemented using ZooKeeeper. As with priority queues, first define a lock node.

Note

源码中有锁的实现
```
There now exists a Lock implementation in ZooKeeper recipes directory. This is distributed with the release -- src/recipes/lock directory of the release artifact.
```
Clients wishing to obtain a lock do the following:

（1）Call create( ) with a pathname of "_locknode_/lock-" and the sequence and ephemeral flags set.

（2）Call getChildren( ) on the lock node without setting the watch flag (this is important to avoid the herd effect).

（3）If the pathname created in step 1 has the lowest sequence number suffix, the client has the lock and the client exits the protocol.

（4）The client calls exists( ) with the watch flag set on the path in the lock directory with the next lowest sequence number.

（5）if exists( ) returns false, go to step 2. Otherwise, wait for a notification for the pathname from the previous step before going to step 2.

The unlock protocol is very simple: clients wishing to release a lock simply delete the node they created in step 1.

Here are a few things to notice:

（1）The removal of a node will only cause one client to wake up since each node is watched by exactly one client. In this way, you avoid the herd effect.

（2）There is no polling or timeouts.

（3）Because of the way you implement locking, it is easy to see the amount of lock contention, break locks, debug locking problems, etc.

#### 1.3 优点
锁安全性高，`ZK`可持久化。

有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。

#### 1.4 缺点
性能开销比较高。因为其需要动态产生、销毁瞬时节点来实现锁功能。

需要对`ZK`的原理有所了解。

### 二 基于 Memcached
#### 2.1 实现原理
`Memcached` 带有 `add` 函数，利用 `add` 函数的特性即可实现分布式锁。`add` 和 `set` 的区别在于：如果多线程并发 `set`，则每个 `set` 都会成功，但最后存储的值以最后的 `set` 的线程为准。而 `add` 的话则相反，`add` 会添加第一个到达的值，并返回`true`，后续的添加则都会返回 `false`。利用该点即可很轻松地实现分布式锁。

#### 2.2 优点
并发高效。

#### 2.3 缺点
（1）`Memcached` 采用列入 `LRU` 置换策略，所以如果内存不够，可能导致缓存中的锁信息丢失。

（2）`Memcached` 无法持久化，一旦重启，将导致信息丢失

### 三 基于 DB
#### 3.1 实现原理
##### 3.1.1 基于数据库表
要实现分布式锁，最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现了。

当我们要锁住某个方法或资源时，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。

创建这样一张数据库表：

```sql
CREATE TABLE `methodLock` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键',
  `method_name` varchar(64) NOT NULL DEFAULT '' COMMENT '锁定的方法名',
  `desc` varchar(1024) NOT NULL DEFAULT '备注信息',
  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '保存数据时间，自动生成',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uidx_method_name` (`method_name `) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='锁定中的方法';
```

当我们想要锁住某个方法时，执行以下`SQL`：
```sql
insert into methodLock(method_name, desc) values (‘method_name’,‘desc’)
```

因为我们对 `method_name` 做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，可以执行方法体内容。

当方法执行完毕之后，想要释放锁的话，需要执行以下`SQL`:
```sql
delete from methodLock where method_name ='method_name'
```
上面这种简单的实现有以下几个问题：

（1）这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。

（2）这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。

（3）这把锁只能是非阻塞的，因为数据的 `insert` 操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。

（4）这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。

当然，我们也可以有其他方式解决上面的问题。

（1）数据库是单点？搞两个数据库，数据之前双向同步。一旦挂掉快速切换到备库上。

（2）没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。

（3）非阻塞的？搞一个`while`循环，直到`insert`成功再返回成功。

（4）非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。

##### 3.1.2 基于数据库排他锁
除了可以通过增删操作数据表中的记录以外，其实还可以借助数据中自带的锁来实现分布式的锁。

我们还用刚刚创建的那张数据库表。可以通过数据库的排他锁来实现分布式锁。

基于`MySQL`的`InnoDB`引擎，可以使用以下方法来实现加锁操作：

```
public boolean lock(){
    connection.setAutoCommit(false)
    while(true){
        try{
            result = select * from methodLock where method_name=xxx for update;
            if(result==null){
                return true;
            }
        }catch(Exception e){

        }
        sleep(1000);
    }
    return false;
}
```

在查询语句后面增加`for update`，数据库会在查询过程中给数据库表增加排他锁。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。

我们可以认为获得排它锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，再通过以下方法解锁：

```java
public void unlock(){
    connection.commit();
}
```

通过`connection.commit()`操作来释放锁。

这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。阻塞锁？ `for update` 语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。但是还是无法直接解决数据库单点和可重入问题。


#### 3.2 优点
直接借助数据库，容易理解。

#### 3.3 缺点
会有各种各样的问题，在解决问题的过程中会使整个方案变得越来越复杂。

操作数据库需要一定的开销，性能问题需要考虑。

### 四 基于 `Redis`
***如果使用基于`Redis`的分布式锁，推荐 `redisson` 实现的 `RedLock`。***
#### 4.1 实现原理
`Redis`为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对`Redis`的连接并不存在竞争关系。

`Redis`提供一些命令`SETNX`，`GETSET`，可以方便实现分布式锁机制。

（1）`setnx` (`lockkey`, 当前时间+过期超时时间) ，如果返回`1`，则获取锁成功；如果返回`0`，则没有获取到锁，转向`2`。

（2）`get(lockkey)`获取值`oldExpireTime` ，并将这个`value`值与当前的系统时间进行比较，如果小于当前系统时间，则认为这个锁已经超时，可以允许别的请求重新获取，转向`3`。

（3）计算`newExpireTime`=当前时间+过期超时时间，然后`getset(lockkey, newExpireTime)` 会返回当前`lockkey`的值`currentExpireTime`。

（4）判断`currentExpireTime`与`oldExpireTime` 是否相等，如果相等，说明当前`getset`设置成功，获取到了锁。如果不相等，说明这个锁又被别的请求获取走了，那么当前请求可以直接返回失败，或者继续重试。

（5）在获取到锁之后，当前线程可以开始自己的业务处理，当处理完毕后，比较自己的处理时间和对于锁设置的超时时间，如果小于锁设置的超时时间，则直接执行`delete`释放锁；如果大于锁设置的超时时间，则不需要再锁进行处理。

注意：为了让分布式锁的算法更稳键些，持有锁的客户端在解锁之前应该再检查一次自己的锁是否已经超时，再去做 `DEL` 操作，因为可能客户端因为某个耗时的操作而挂起，操作完的时候锁因为超时已经被别人获得，这时就不必解锁了。

#### 4.2 Redlock

##### 4.2.1 安全和可靠性保证
在描述我们的设计之前，我们想先提出三个属性，这三个属性在我们看来，是实现高效分布式锁的基础。

+ 一致性：互斥，不管任何时候，只有一个客户端能持有同一个锁。
+ 分区可容忍性：不会死锁，最终一定会得到锁，就算一个持有锁的客户端宕掉或者发生网络分区。
+ 可用性：只要大多数Redis节点正常工作，客户端应该都能获取和释放锁。

#### 4.2.2 为什么基于故障切换的方案不够好
为了理解我们想要提高的到底是什么，我们先看下当前大多数基于`Redis`的分布式锁三方库的现状。

用`Redis`来实现分布式锁最简单的方式就是在实例里创建一个键值，创建出来的键值一般都是有一个超时时间的（这个是`Redis`自带的超时特性），所以每个锁最终都会释放（参见前文属性2）。而当一个客户端想要释放锁时，它只需要删除这个键值即可。 表面来看，这个方法似乎很管用，但是这里存在一个问题：在我们的系统架构里存在一个单点故障，如果`Redis`的`master`节点宕机了怎么办呢？有人可能会说：加一个`slave`节点！在`master`宕机时用`slave`就行了！但是其实这个方案明显是不可行的，因为这种方案无法保证第`1`个安全互斥属性，因为`Redis`的复制是异步的。 总的来说，这个方案里有一个明显的竞争条件（`race condition`），举例来说：
```
客户端A在master节点拿到了锁。
master节点在把A创建的key写入slave之前宕机了。
slave变成了master节点
B也得到了和A还持有的相同的锁（因为原来的slave里还没有A持有锁的信息）
```

当然，在某些特殊场景下，前面提到的这个方案则完全没有问题，比如在宕机期间，多个客户端允许同时都持有锁，如果你可以容忍这个问题的话，那用这个基于复制的方案就完全没有问题，否则的话我们还是建议你采用这篇文章里接下来要描述的方案。

#### 4.2.3 采用单实例的正确实现
在讲述如何用其他方案突破单实例方案的限制之前，让我们先看下是否有什么办法可以修复这个简单场景的问题，因为这个方案其实如果可以忍受竞争条件的话是有望可行的，而且单实例来实现分布式锁是我们后面要讲的算法的基础。

要获得锁，要用下面这个命令：
> SET resource_name my_random_value NX PX 30000

这个命令的作用是在只有这个key不存在的时候才会设置这个key的值（NX选项的作用），超时时间设为30000毫秒（PX选项的作用） 这个key的值设为“my_random_value”。这个值必须在所有获取锁请求的客户端里保持唯一。 基本上这个随机值就是用来保证能安全地释放锁，我们可以用下面这个Lua脚本来告诉Redis：删除这个key当且仅当这个key存在而且值是我期望的那个值。

```java
if redis.call("get",KEYS[1]) == ARGV[1] then
        return redis.call("del",KEYS[1])
    else
        return 0
    end
```

这个很重要，因为这可以避免误删其他客户端得到的锁，举个例子，一个客户端拿到了锁，被某个操作阻塞了很长时间，过了超时时间后自动释放了这个锁，然后这个客户端之后又尝试删除这个其实已经被其他客户端拿到的锁。所以单纯的用DEL指令有可能造成一个客户端删除了其他客户端的锁，用上面这个脚本可以保证每个客户单都用一个随机字符串`签名`了，这样每个锁就只能被获得锁的客户端删除了。

这个随机字符串应该用什么生成呢？我假设这是从`/dev/urandom`生成的`20`字节大小的字符串，但是其实你可以有效率更高的方案来保证这个字符串足够唯一。比如你可以用`RC4`加密算法来从`/dev/urandom`生成一个伪随机流。还有更简单的方案，比如用毫秒的`unix`时间戳加上客户端`id`，这个也许不够安全，但是也许在大多数环境下已经够用了。

`key`值的超时时间，也叫做`锁有效时间`。这个是锁的自动释放时间，也是一个客户端在其他客户端能抢占锁之前可以执行任务的时间，这个时间从获取锁的时间点开始计算。 所以现在我们有很好的获取和释放锁的方式，在一个非分布式的、单点的、保证永不宕机的环境下这个方式没有任何问题，接下来我们看看无法保证这些条件的分布式环境下我们该怎么做。

#### 4.2.4 `Redlock`算法

在分布式版本的算法里我们假设我们有`N`个`Redis master`节点，这些节点都是完全独立的，我们不用任何复制或者其他隐含的分布式协调算法。我们已经描述了如何在单节点环境下安全地获取和释放锁。因此我们理所当然地应当用这个方法在每个单节点里来获取和释放锁。在我们的例子里面我们把`N`设成`5`，这个数字是一个相对比较合理的数值，因此我们需要在不同的计算机或者虚拟机上运行`5`个`master`节点来保证他们大多数情况下都不会同时宕机。一个客户端需要做如下操作来获取锁：

（1）获取当前时间（单位是毫秒）。

（2）轮流用相同的`key`和随机值在`N`个节点上请求锁，在这一步里，客户端在每个`master`上请求锁时，会有一个和总的锁释放时间相比小的多的超时时间。比如如果锁自动释放时间是`10`秒钟，那每个节点锁请求的超时时间可能是`5-50`毫秒的范围，这个可以防止一个客户端在某个宕掉的`master`节点上阻塞过长时间，如果一个`master`节点不可用了，我们应该尽快尝试下一个`master`节点。

（3）客户端计算第二步中获取锁所花的时间，只有当客户端在大多数`master`节点上成功获取了锁（在这里是`3`个），而且总共消耗的时间不超过锁释放时间，这个锁就认为是获取成功了。

（4）如果锁获取成功了，那现在锁自动释放时间就是最初的锁释放时间减去之前获取锁所消耗的时间。

（5）如果锁获取失败了，不管是因为获取成功的锁不超过一半（`N/2+1`)还是因为总消耗时间超过了锁释放时间，客户端都会到每个`master`节点上释放锁，即便是那些他认为没有获取成功的锁。

#### 4.2.5 这个算法是否是异步的？
这个算法是基于一个假设：虽然不存在可以跨进程的同步时钟，但是不同进程时间都是以差不多相同的速度前进，这个假设不一定完全准确，但是和自动释放锁的时间长度相比不同进程时间前进速度差异基本是可以忽略不计的。这个假设就好比真实世界里的计算机：每个计算机都有本地时钟，但是我们可以说大部分情况下不同计算机之间的时间差是很小的。 现在我们需要更细化我们的锁互斥规则，只有当客户端能在T时间内完成所做的工作才能保证锁是有效的（详见算法的第3步），`T`的计算规则是锁失效时间`T1`减去一个用来补偿不同进程间时钟差异的`delta`值（一般只有几毫秒而已） 如果想了解更多基于有限时钟差异的类似系统，可以参考这篇有趣的文章：《`Leases: an efficient fault-tolerant mechanism for distributed file cache consistency`.》

#### 4.2.6 失败的重试
当一个客户端获取锁失败时，这个客户端应该在一个随机延时后进行重试，之所以采用随机延时是为了避免不同客户端同时重试导致谁都无法拿到锁的情况出现。同样的道理客户端越快尝试在大多数`Redis`节点获取锁，出现多个客户端同时竞争锁和重试的时间窗口越小，可能性就越低，所以最完美的情况下，客户端应该用多路传输的方式同时向所有`Redis`节点发送`SET`命令。 这里非常有必要强调一下客户端如果没有在多数节点获取到锁，一定要尽快在获取锁成功的节点上释放锁，这样就没必要等到`key`超时后才能重新获取这个锁（但是如果网络分区的情况发生而且客户端无法连接到`Redis`节点时，会损失等待`key`超时这段时间的系统可用性）

#### 4.2.7 释放锁
释放锁比较简单，因为只需要在所有节点都释放锁就行，不管之前有没有在该节点获取锁成功。

#### 4.2.8 安全性的论证
这个算法到底是不是安全的呢？我们可以观察不同场景下的情况来理解这个算法为什么是安全的。 开始之前，让我们假设客户端可以在大多数节点都获取到锁，这样所有的节点都会包含一个有相同存活时间的`key`。但是需要注意的是，这个`key`是在不同时间点设置的，所以这些`key`也会在不同的时间超时，但是我们假设最坏情况下第一个`key`是在`T1`时间设置的（客户端连接到第一个服务器时的时间），最后一个`key`是在`T2`时间设置的（客户端收到最后一个服务器返回结果的时间），从`T2`时间开始，我们可以确认最早超时的`key`至少也会存在的时间为`MIN_VALIDITY=TTL-(T2-T1)-CLOCK_DRIFT`，`TTL`是锁超时时间、`（T2-T1）`是最晚获取到的锁的耗时，`CLOCK_DRIFT`是不同进程间时钟差异，这个是用来补偿前面的`(T2-T1)`。其他的`key`都会在这个时间点之后才会超时，所以我们可以确定这些`key`在这个时间点之前至少都是同时存在的。

在大多数节点的`key`都`set`了的时间段内，其他客户端无法抢占这个锁，因为在`N/2+1`个客户端的`key`已经存在的情况下不可能再在`N/2+1`个客户端上获取锁成功，所以如果一个锁获取成功了，就不可能同时重新获取这个锁成功（不然就违反了分布式锁互斥原则），然后我们也要确保多个客户端同时尝试获取锁时不会都同时成功。 如果一个客户端获取大多数节点锁的耗时接近甚至超过锁的最大有效时间时（就是我们为`SET`操作设置的`TTL`值），那么系统会认为这个锁是无效的同时会释放这些节点上的锁，所以我们仅仅需要考虑获取大多数节点锁的耗时小于有效时间的情况。在这种情况下，根据我们前面的证明，在`MIN_VALIDITY`时间内，没有客户端能重新获取锁成功，所以多个客户端都能同时成功获取锁的结果，只会发生在多数节点获取锁的时间都大大超过`TTL`时间的情况下，实际上这种情况下这些锁都会失效 。

#### 4.2.9 性能论证
这个系统的性能主要基于以下三个主要特征：

（1）锁自动释放的特征（超时后会自动释放），一定时间后某个锁都能被再次获取。

（2）客户端通常会在不再需要锁或者任务执行完成之后主动释放锁，这样我们就不用等到超时时间会再去获取这个锁。

（3）当一个客户端需要重试获取锁时，这个客户端会等待一段时间，等待的时间相对来说会比我们重新获取大多数锁的时间要长一些，这样可以降低不同客户端竞争锁资源时发生死锁的概率。

然而，我们在网络分区时要损失TTL的可用性时间，所以如果网络分区持续发生，这个不可用会一直持续。这种情况在每次一个客户端获取到了锁并在释放锁之前被网络分区了时都会出现。

基本来说，如果持续的网络分区发生的话，系统也会在持续不可用。

#### 4.2.10 性能、故障恢复和`fsync`
很多使用`Redis`做锁服务器的用户在获取锁和释放锁时不止要求低延时，同时要求高吞吐量，也即单位时间内可以获取和释放的锁数量。为了达到这个要求，一定会使用多路传输来和N个服务器进行通信以降低延时（或者也可以用假多路传输，也就是把`socket`设置成非阻塞模式，发送所有命令，然后再去读取返回的命令，假设说客户端和不同`Redis`服务节点的网络往返延时相差不大的话）。

然后如果我们想让系统可以自动故障恢复的话，我们还需要考虑一下信息持久化的问题。

为了更好的描述问题，我们先假设我们`Redis`都是配置成非持久化的，某个客户端拿到了总共`5`个节点中的`3`个锁，这三个已经获取到锁的节点中随后重启了，这样一来我们又有`3`个节点可以获取锁了（重启的那个加上另外两个），这样一来其他客户端又可以获得这个锁了，这样就违反了我们之前说的锁互斥原则了。

如果我们启用`AOF`持久化功能，情况会好很多。举例来说，我们可以发送`SHUTDOWN`命令来升级一个`Redis`服务器然后重启之，因为`Redis`超时时效是语义层面实现的，所以在服务器关掉期间时超时时间还是算在内的，我们所有要求还是满足了的。然后这个是基于我们做的是一次正常的`shutdown`，但是如果是断电这种意外停机呢？如果`Redis`是默认地配置成每秒在磁盘上执行一次`fsync`同步文件到磁盘操作，那就可能在一次重启后我们锁的`key`就丢失了。理论上如果我们想要在所有服务重启的情况下都确保锁的安全性，我们需要在持久化设置里设置成永远执行`fsync`操作，但是这个反过来又会造成性能远不如其他同级别的传统用来实现分布式锁的系统。 然后问题其实并不像我们第一眼看起来那么糟糕，基本上只要一个服务节点在宕机重启后不去参与现在所有仍在使用的锁，这样正在使用的锁集合在这个服务节点重启时，算法的安全性就可以维持，因为这样就可以保证正在使用的锁都被所有没重启的节点持有。 为了满足这个条件，我们只要让一个宕机重启后的实例，至少在我们使用的最大`TTL`时间内处于不可用状态，超过这个时间之后，所有在这期间活跃的锁都会自动释放掉。 使用延时重启的策略基本上可以在不适用任何`Redis`持久化特性情况下保证安全性，然后要注意这个也必然会影响到系统的可用性。举个例子，如果系统里大多数节点都宕机了，那在`TTL`时间内整个系统都处于全局不可用状态（全局不可用的意思就是在获取不到任何锁）。

#### 4.2.11 扩展锁来使得算法更可靠
如果客户端做的工作都是由一些小的步骤组成，那么就有可能使用更小的默认锁有效时间，而且扩展这个算法来实现一个锁扩展机制。基本上，客户端如果在执行计算期间发现锁快要超时了，客户端可以给所有服务实例发送一个`Lua`脚本让服务端延长锁的时间，只要这个锁的`key`还存在而且值还等于客户端获取时的那个值。 客户端应当只有在失效时间内无法延长锁时再去重新获取锁（基本上这个和获取锁的算法是差不多的） 然而这个并不会对从本质上改变这个算法，所以最大的重新获取锁数量应该被设置成合理的大小，不然性能必然会受到影响。

因为`Redlock`的安全性（`safety property`）对系统的时钟有比较强的依赖，一旦系统的时钟变得不准确，算法的安全性也就保证不了了。

`Martin`在这里其实是要指出分布式算法研究中的一些基础性问题，或者说一些常识问题，即好的分布式算法应该基于异步模型（`asynchronous model`），算法的安全性不应该依赖于任何记时假设(`timing assumption`)。

在异步模型中：进程可能`pause`任意长的时间，消息可能在网络中延迟任意长的时间，甚至丢失，系统时钟也可能以任意方式出错。一个好的分布式算法，这些因素不应该影响它的安全性（`safety propert`），只可能影响到它的活性（`liveness property`），也就是说，即使在非常极端的情况下（比如系统时钟严重错误），算法顶多是不能在有限的时间内给出结果而已，而不应该给出错误的结果。

这样的算法在现实中是存在的，像比较著名的`Paxos`，或`Raft`。

但显然按这个标准的话，`Redlock`的安全性级别是达不到的。

对锁的用途的区分。他把锁的用途分为两种：

+ 为了效率（`efficiency`），协调各个客户端避免做重复的工作。即使锁偶尔失效了，只是可能把某些操作多做一遍而已，不会产生其它的不良后果。比如重复发送了一封同样的email。

+ 为了正确性（`correctness`）。在任何情况下都不允许锁失效的情况发生，因为一旦发生，就可能意味着数据不一致（`inconsistency`），数据丢失，文件损坏，或者其它严重的问题。

按照锁的两种用途，如果仅是为了效率（`efficiency`），那么你可以自己选择你喜欢的一种分布式锁的实现。当然，你需要清楚地知道它在安全性上有哪些不足，以及它会带来什么后果。

而如果你是为了正确性（`correctness`），那么请慎之又慎。

### 5. Zookeeper vs Redis
（1）`Zookeeper` 与 `Redis` 都有可能因为锁过期（`Zookeeper`的连接失效，`Redis`的过期时间已到），造成多个实例访问临界区。

（2）`Redis`锁的性能好。

（3）`Zookeeper` 锁的安全性高。

### 我的理解

（1）需要一个资源中心。

（2）问题：锁由谁释放？
释放动作是原子的，与获取一样。

（3）如果只有锁的拥有者Owner能释放，那么当Owner等待或死亡，锁就会一直不释放。于是需要其他竞争者也能释放锁。

（4）如果竞争者能释放锁，那么需要一个规则，否则多个竞争者获得同一个锁。

（5）规则对所有竞争者可见。

（6）执行业务需要时间，超时是一种规则。

（7）超时的怎么设定？时间短，可能多个竞争者获得同一个锁；时间长，需要Owner释放。

（8）说到底，还是正确性与活性的权衡取舍。

### 参考资料

#### 1. 分布式锁的几种实现方式
https://www.cnblogs.com/garfieldcgf/p/6380816.html

#### 2. 数据库：Mysql中“select ... for update”排他锁分析
https://blog.csdn.net/claram/article/details/54023216

#### 3. 基于Redis的分布式锁到底安全吗
上篇：http://zhangtielei.com/posts/blog-redlock-reasoning.html

下篇：http://zhangtielei.com/posts/blog-redlock-reasoning-part2.html

#### 4. ZooKeeper Recipes and Solutions
http://zookeeper.apache.org/doc/current/recipes.html

#### 5. Distributed locks with Redis
https://github.com/antirez/redis-doc/blob/master/topics/distlock.md

#### 6. 用Redis构建分布式锁-RedLock(真分布)
https://www.cnblogs.com/ironPhoenix/p/6048467.html

#### 7. redisson，基于Redis的封装
官网：https://redisson.org/

github：https://github.com/redisson/redisson

#### 8. curator，基于Zookeeper的封装
官网：http://curator.apache.org/

github：https://github.com/apache/curator

#### 9. 【原创】分布式之抉择分布式锁
http://www.cnblogs.com/rjzheng/p/9310976.html
