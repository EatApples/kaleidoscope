# 为什么这么设计系列文章

地址：https://draveness.me/whys-the-design/
作者：draveness

### 001 为什么 TCP 建立连接需要三次握手

很多人尝试回答或者思考这个问题的时候其实关注点都放在了三次握手中的三次上面，这确实很重要，但是如果重新审视这个问题，我们对于『什么是连接』真的清楚？只有知道连接的定义，我们才能去尝试回答为什么 TCP 建立连接需要三次握手。

The reliability and flow control mechanisms described above require that TCPs initialize and maintain certain status information for each data stream. The combination of this information, including sockets, sequence numbers, and window sizes, is called a connection.

RFC 793 - Transmission Control Protocol 文档中非常清楚地定义了 TCP 中的连接是什么，我们简单总结一下：用于保证可靠性和流控制机制的信息，包括 Socket、序列号以及窗口大小叫做连接。

到这里，我们将原有的问题转换成了『为什么需要通过三次握手才可以初始化 Sockets、窗口大小和初始序列号？』

（1）历史连接

RFC 793 - Transmission Control Protocol 其实就指出了 TCP 连接使用三次握手的首要原因 —— 为了阻止历史的重复连接初始化造成的混乱问题，防止使用 TCP 协议通信的双方建立了错误的连接。

The principle reason for the three-way handshake is to prevent old duplicate connection initiations from causing confusion.

（2）初始序列号

另一个使用三次握手的重要的原因就是通信双方都需要获得一个用于发送信息的初始化序列号，作为一个可靠的传输层协议，TCP 需要在不稳定的网络环境中构建一个可靠的传输层

除此之外，网络作为一个分布式的系统，其中并不存在一个用于计数的全局时钟，而 TCP 可以通过不同的机制来初始化序列号，作为 TCP 连接的接收方我们无法判断对方传来的初始化序列号是否过期，所以我们需要交由对方来判断，TCP 连接的发起方可以通过保存发出的序列号判断连接是否过期，如果让接收方来保存并判断序列号却是不现实的，这也再一次强化了我们在上一节中提出的观点 —— 避免历史错连接的初始化

（3）总结

TCP 建立连接时通过三次握手可以有效地避免历史错误连接的建立，减少通信双方不必要的资源消耗，三次握手能够帮助通信双方获取初始化序列号，它们能够保证数据包传输的不重不丢，还能保证它们的传输顺序，不会因为网络传输的问题发生混乱，到这里不使用『两次握手』和『四次握手』的原因已经非常清楚了：

『两次握手』：无法避免历史错误连接的初始化，浪费接收方的资源；
『四次握手』：TCP 协议的设计可以让我们同时传递 ACK 和 SYN 两个控制信息，减少了通信次数，所以不需要使用更多的通信次数传输相同的信息；

### 002 为什么使用通信来共享内存

『不要通过共享内存来通信，我们应该使用通信来共享内存』，这是一句使用 Go 语言编程的人经常能听到的观点。

使用通信来共享内存其实不只是 Go 语言推崇的哲学，更为古老的 Erlang 语言其实也遵循了同样的设计，然而这两者在具体实现上其实有一些不同，其中前者使用通信顺序进程（Communication Sequential Process），而后者使用 Actor 模型进行设计；这两种不同的并发模型都是『使用通信来共享内存』的具体实现，它们的主要作用都是在不同的线程或者协程之间交换信息。

从本质上来看，计算机上线程和协程同步信息其实都是通过『共享内存』来进行的，因为无论是哪种通信模型，线程或者协程最终都会从内存中获取数据，所以更为准确的说法是『为什么我们使用发送消息的方式来同步信息，而不是多个线程或者协程直接共享内存？』

Go 语言并发模型的设计深受 CSP 模型的影响，我们简单总结一下为什么我们应该使用通信的方式来共享内存。

Do not communicate by sharing memory; instead, share memory by communicating.

（1）首先，使用发送消息来同步信息相比于直接使用共享内存和互斥锁是一种更高级的抽象，使用更高级的抽象能够为我们在程序设计上提供更好的封装，让程序的逻辑更加清晰；

（2）其次，消息发送在解耦方面与共享内存相比也有一定优势，我们可以将线程的职责分成生产者和消费者，并通过消息传递的方式将它们解耦，不需要再依赖共享内存；

（3）最后，Go 语言选择消息发送的方式，通过保证同一时间只有一个活跃的线程能够访问数据，能够从设计上天然地避免线程竞争和数据冲突的问题；

### 003 为什么 Redis 选择单线程模型

Redis 作为一个内存服务器，它需要处理很多来自外部的网络请求，它使用 I/O 多路复用机制同时监听多个文件描述符的可读和可写状态，一旦收到网络请求就会在内存中快速处理，由于绝大多数的操作都是纯内存的，所以处理的速度会非常地快。

在 Redis 4.0 之后的版本，情况就有了一些变动，新版的 Redis 服务在执行一些命令时就会使用『主处理线程』之外的其他线程，例如 UNLINK、FLUSHALL ASYNC、FLUSHDB ASYNC 等非阻塞的删除操作。

（1）使用单线程模型能带来更好的可维护性，方便开发和调试；

（2）使用单线程模型也能并发的处理客户端的请求；

（3）Redis 服务中运行的绝大多数操作的性能瓶颈都不是 CPU；

上述三个原因中的最后一个是最终使用单线程模型的决定性因素，其他的两个原因都是使用单线程模型额外带来的好处，在这里我们会按顺序介绍上述的几个原因。

### 004 为什么你应该使用 Git 进行版本控制

Linus 在 2007 年的 Google Talk 上曾经介绍过版本控制系统必须具有的三个特性：

（1）分布式：版本控制系统中的开发模型必须是分布式的；分布式的方式更加符合版本控制系统的工作场景；

（2）高性能：版本控制系统必须提供足够好的性能支持；性能的提升能够鼓励我们做出成本更低的操作；

（3）可靠性：版本控制系统必须保证文件的完整性，提供来自于数据丢失或者损坏的保护；可靠性能帮助我们能够及时发现数据因磁盘故障而丢失或者损坏；

### 005 为什么 DNS 使用 UDP 协议

（1）DNS 在设计之初就在区域传输中引入了 TCP 协议，在查询中使用 UDP 协议；

（2）当 DNS 超过了 512 字节的限制，我们第一次在 DNS 协议中明确了『当 DNS 查询被截断时，应该使用 TCP 协议进行重试』这一规范；

（3）随后引入的 EDNS 机制允许我们使用 UDP 最多传输 4096 字节的数据，但是由于 MTU 的限制导致的数据分片以及丢失，使得这一特性不够可靠；

（4）在最近的几年，我们重新规定了 DNS 应该同时支持 UDP 和 TCP 协议，TCP 协议也不再只是重试时的选择；

这篇文章已经详细介绍了 DNS 的历史以及选择不同协议时考虑的关键点，在这里我们重新回顾一下 DNS 查询选择 UDP 或者 TCP 两种不同协议时的主要原因：

UDP 协议

- DNS 查询的数据包较小、机制简单；
- UDP 协议的额外开销小、有着更好的性能表现；

TCP 协议

- DNS 查询由于 DNSSEC 和 IPv6 的引入迅速膨胀，导致 DNS 响应经常超过 MTU 造成数据的分片和丢失，我们需要依靠更加可靠的 TCP 协议完成数据的传输；
- 随着 DNS 查询中包含的数据不断增加，TCP 协议头以及三次握手带来的额外开销比例逐渐降低，不再是占据总传输数据大小的主要部分；

无论是选择 UDP 还是 TCP，最核心的矛盾就在于需要传输的数据包大小，如果数据包小到一定程度，UDP 协议绝对最佳的选择，但是当数据包逐渐增大直到突破 512 字节以及 MTU 1500 字节的限制时，我们也只能选择使用更可靠的 TCP 协议来传输 DNS 查询和响应。

### 006 为什么使用 MD5 存储密码非常危险

很多开发者对于 MD5 的作用和定义都有着非常大的误解，MD5 并不是一种加密算法，而是一种摘要算法。

在任何场景下，我们都应该避免 MD5 的使用，可以选择更好的摘要算法替代 MD5，例如 SHA256、SHA512。

『为什么 MD5 不能用于存储密码』，对于这个问题有一个最简单的答案，也就是 MD5 不够安全。

哈希加盐的方式确实能够增加攻击者的成本，但是今天来看还远远不够，我们需要一种更加安全的方式来存储用户的密码，这也就是今天被广泛使用的 bcrypt，使用 bcrypt 相比于直接使用哈希加盐是一种更加安全的方式，也是我们目前推荐使用的方法，为了增加攻击者的成本，bcrypt 引入了计算成本这一可以调节的参数，能够调节执行 bcrypt 函数的成本。

“bcrypt was designed for password hashing hence it is a slow algorithm. This is good for password hashing as it reduces the number of passwords by second an attacker could hash when crafting a dictionary attack. "

bcrypt 这一算法就是为哈希密码而专门设计的，所以它是一个执行相对较慢的算法，这也就能够减少攻击者每秒能够处理的密码数量，从而避免攻击者的字典攻击。

### 007 为什么基础服务不应该高可用

我们将从以下的三个方面依次分析『为什么基础服务不应该高可用』：

（1）墨菲定律：凡是可能出错的事情就一定会出错；
（2）边际成本：成本因素有时是技术决策的最关键因素；
（3）异构服务：只有使用更多异构的副本才能保证更高的可用性；

### 008 为什么总是需要无意义的 ID

我们将开始分析为什么很多业务或者场景中都需要一个唯一 ID，例如：消息队列、TCP 通信等场景，我们可以将这一问题归结到两个原因上：

（1）需要通过唯一的标识符对数据或者事件进行识别或者去重；
（2）只有无意义的标识符才会绝对唯一的，任何携带其他信息的标识都可能重复；

### 009 为什么 MySQL 使用 B+ 树

在具体分析 InnoDB 使用 B+ 树背后的原因之前，我们需要为 B+ 树找几个『假想敌』，因为如果我们只有一个选择，那么选择 B+ 树也并不值得讨论，找到的两个假想敌就是 B 树和哈希，相信这也是很多人会在面试中真实遇到的问题，我们就以这两种数据结构为例，分析比较 B+ 树的优点。

我们将通过以下的两个方面介绍 InnoDB 这样选择的原因。

（1）InnoDB 需要支持的场景和功能需要在特定查询上拥有较强的性能；
（2）CPU 将磁盘上的数据加载到内存中需要花费大量的时间，这使得 B+ 树成为了非常好的选择；

我们在这里重新回顾一下 MySQL 默认的存储引擎选择 B+ 树而不是哈希或者 B 树的原因：

哈希虽然能够提供 O(1) 的单数据行操作性能，但是对于范围查询和排序却无法很好地支持，最终导致全表扫描；

B 树能够在非叶节点中存储数据，但是这也导致在查询连续数据时可能会带来更多的随机 I/O，而 B+ 树的所有叶节点可以通过指针相互连接，能够减少顺序遍历时产生的额外随机 I/O；

### 010 为什么 Redis 快照使用子进程

SAVE 命令在执行时会直接阻塞当前的线程，由于 Redis 是 单线程 的，所以 SAVE 命令会直接阻塞来自客户端的所有其他请求，这在很多时候对于需要提供较强可用性保证的 Redis 服务都是无法接受的。

我们往往需要 BGSAVE 命令在后台生成 Redis 全部数据对应的 RDB 文件，当我们使用 BGSAVE 命令时，Redis 会立刻 fork 出一个子进程，子进程会执行『将内存中的数据以 RDB 格式保存到磁盘中』这一过程，而 Redis 服务在 BGSAVE 工作期间仍然可以处理来自客户端的请求。

（1）通过 fork 创建的子进程能够获得和父进程完全相同的内存空间，父进程对内存的修改对于子进程是不可见的，两者不会相互影响；

（2）通过 fork 创建子进程时不会立刻触发大量内存的拷贝，内存在被修改时会以页为单位进行拷贝，这也就避免了大量拷贝内存而带来的性能问题；

### 011 为什么 MongoDB 使用 B 树

MongoDB 和 MySQL 在多个不同数据结构之间选择的最终目的就是减少查询需要的随机 IO 次数，MySQL 认为遍历数据的查询是常见的，所以它选择 B+ 树作为底层数据结构，而舍弃了通过非叶节点存储数据这一特性。

虽然遍历数据的查询是相对常见的，但是 MongoDB 认为查询单个数据记录远比遍历数据更加常见，由于 B 树的非叶结点也可以存储数据，所以查询一条数据所需要的平均随机 IO 次数会比 B+ 树少，使用 B 树的 MongoDB 在类似场景中的查询速度就会比 MySQL 快。

既然 MongoDB 认为查询单个数据记录远比遍历数据的查询更加常见，那为什么不使用哈希作为底层的数据结构呢？

如果我们使用哈希，那么对于所有单条记录查询的复杂度都会是 O(1)，但是遍历数据的复杂度就是 O(n)；如果使用 B+ 树，那么单条记录查询的复杂度是 O(log n)，遍历数据的复杂度就是 O(log n) + X，这两种不同的数据结构一种提供了最好的单记录查询性能，一种提供了最好的遍历数据的性能，但是这都不能满足 MongoDB 面对的场景 —— 单记录查询非常常见，但是对于遍历数据也需要有相对较好的性能支持，哈希这种性能表现较为极端的数据结构往往只能在简单、极端的场景下使用。

MongoDB 最终选择使用 B 树的两个原因：

（1）MySQL 使用 B+ 树是因为数据的遍历在关系型数据库中非常常见，它经常需要处理各个表之间的关系并通过范围查询一些数据；但是 MongoDB 作为面向文档的数据库，与数据之间的关系相比，它更看重以文档为中心的组织方式，所以选择了查询单个文档性能较好的 B 树，这个选择对遍历数据的查询也可以保证可以接受的时延；

（2）LSM 树（Log-structured merge-tree）是一种专门用来优化写入的数据结构，它将随机写变成了顺序写显著地提高了写入性能，但是却牺牲了读的效率，这与大多数场景需要的特点是不匹配的，所以 MongoDB 最终还是选择读取性能更好的 B 树作为默认的数据结构；

### 012 为什么 TCP 协议有性能问题

本文将分析在弱网环境下（丢包率高）影响 TCP 性能的三个原因：

（1）TCP 的拥塞控制在发生丢包时会进行退让，减少能够发送的数据段数量，但是丢包并不一定意味着网络拥塞，更多的可能是网络状况较差；
（2）TCP 的三次握手带来了额外开销，这些开销不只包括需要传输更多的数据，还增加了首次传输数据的网络延迟；
（3）TCP 的重传机制在数据包丢失时可能会重新传输已经成功接收的数据段，造成带宽的浪费；

在上述的三个原因中，拥塞控制算法是导致 TCP 在弱网环境下有着较差表现的首要原因，三次握手和累计应答两者的影响依次递减，但是也加剧了 TCP 的性能问题。

早期互联网的大多数计算设备都通过有线网络连接，出现网络不稳定的可能性也比较低，所以 TCP 协议的设计者认为丢包意味着网络出现拥塞，一旦发生丢包，客户端疯狂重试就可能导致互联网的拥塞崩溃，所以发明了拥塞控制算法来解决该问题。

但是如今的网络环境更加复杂，无线网络的引入导致部分场景下的网络不稳定成了常态，所以丢包并不一定意味着网络拥堵，如果使用更加激进的策略传输数据，在一些场景下会得到更好的效果。

为了解决 TCP 的性能问题，目前业界有两种解决方案：

（1）使用 UDP 构建性能更加优异、更灵活的传输协议，例如：QUIC 等；

（2）通过不同的手段优化 TCP 协议的性能，例如：选择性 ACK（Selective ACK, SACK），TCP 快开启（TCP Fast Open, TFO）；

由于 TCP 协议在操作系统内核中，不利于协议的更新，所以第一种方案目前发展的更好，HTTP/3 就使用了 QUIC 作为传输协议。

### 013 为什么 UDP 头只有 8 个字节

由于 UDP 协议既不需要保证送达，也不需要保证顺序，所以它没有 TCP 协议那么复杂。TCP 协议中的三次握手、拥塞控制算法和重传策略等机制都是为了提供可靠性所付出的必要代价，但是 UDP 协议不需要这些策略，它只尽力保证数据报的送达。

我们都说 UDP 协议是传输层协议，但是真正在主机间完成『数据传输』工作的是 IP 协议，UDP 协议只起到了定位具体进程的作用。

IP 协议是 TCP/IP 协议栈的核心成员，它不保证端到端数据的可靠性和顺序，也不包含流控制等机制，其作用就是从来源向目的地传输数据包。

UDP 协议利用下层的 IP 协议提供基本的数据传输能力，它的作用就是引入端口号的概念让同一主机可以同时提供对外多个服务，由于不保证可靠性，所以协议本身只占用 8 个字节。

TCP 和 UDP 的端口号是主机和进程的中间层，进程和端口号既可以是一对一的关系，也可以是一对多的关系，端口号的引入可以让同一个主机上的多个进程对外提供服务，也可以让一个进程对外提供多个服务。有了端口号，想要访问主机服务的请求也不需要使用进程标识符等方式定位提供服务的具体进程。

在理想情况下，我们可以在 IP 协议上构建新的传输层协议实现特定的需求，不过在实际操作中由于协议号的限制，新的传输层协议无法被大量部署的网络地址转换（Network address translation，NAT）设备识别和支持，所以使用这种方式构建新的传输层协议在实践中难以落实。

### 014 为什么 Go 语言没有泛型

既然泛型能够增强语言的表达能力，提升工程师的效率，那么为什么 Go 语言到目前为止也不支持泛型呢？本文总结了两个原因：

（1）泛型困境使我们必须在开发效率、编译速度和运行速度三者中选择两个；
（2）目前社区中的 Go 语言方案都是有缺陷的，而 Go 团队认为泛型的支持不够紧急；

弱开发效率：C 语言是系统级的编程语言，它没有支持泛型，本身提供的抽象能力非常有限。这样做的结果是牺牲了程序员的开发效率，与 Go 语言目前的做法一样，它们都需要手动实现不同类型的相同逻辑。但是不引入泛型的好处也显而易见 —— 降低了编译器实现的复杂度，也能保证源代码的编译速度；

弱编译速度：C++ 与 C 语言的选择完全不同，它使用编译期间类型特化实现泛型，提供了非常强大的抽象能力。虽然提高了程序员的开发效率，不再需要手写同一逻辑的相似实现，但是编译器的实现变得非常复杂，泛型展开会生成的大量重复代码也会导致最终的二进制文件膨胀和编译缓慢，我们往往需要链接器来解决代码重复的问题；

弱运行速度：Java 在 1.5 版本引入了泛型，它的泛型是用类型擦除实现的。Java 的泛型只是在编译期间用于检查类型的正确，为了保证与旧版本 JVM 的兼容，类型擦除会删除泛型的相关信息，导致其在运行时不可用。编译器会插入额外的类型转换指令，与 C 语言和 C++ 在运行前就已经实现或者生成代码相比，Java 类型的装箱和拆箱会降低程序的执行效率；

### 015 为什么数据库会丢失数据

无论是开源数据库还是云服务商提供的服务，都有可能发生数据丢失的。本文将数据库丢失数据的原因归结到以下的几个方面，我们将详细展开介绍这些原因：

（1）人为因素导致的运维和配置错误是数据库丢失数据的首要原因；
（2）数据库存储数据使用的磁盘损坏导致数据丢失；
（3）数据库的功能和实现复杂，数据没有及时刷入磁盘就有丢失的风险；

### 016 为什么比特币可以防篡改

比特币网络主要会通过以下两种技术保证用户签发的交易和历史上发生的交易不会被攻击者篡改：

（1）非对称加密可以保证攻击者无法伪造账户所有者的签名；
（2）共识算法可以保证网络中的历史交易不会被攻击者替换；

比特币使用如下的规则让多个节点实现分布式一致性：

- 引入工作量证明 — 让节点在提交新的区块之前计算满足特定条件的哈希，取代传统分布式一致性算法中，一人一票（或者一节点一票）的设定；
- 引入最长链是主链的设定 — 只有主链上的交易才被认为是合法交易；
- 引入激励 — 提交区块的节点可以获得比特币奖励；

### 017 为什么 TCP/IP 协议会拆分数据

IP 协议是用于传输数据包的协议，作为网络层协议，它能提供数据的路由和寻址功能，让数据通过网络到达目的地。不同设备之间传输数据前，需要先确定一个 IP 数据包的大小上限，即最大传输单元（Maximum transmission unit，即 MTU），MTU 是 IP 数据包能够传输的数据上限。

MTU 的值不是越大越好，更大的 MTU 意味着更低的额外开销，更小的 MTU 意味着更低的网络延迟。

路径最大传输单元发现（Path MTU Discovery，PMTUD）是用来确定两个主机传输路径 MTU 的机制，它的工作原理如下：

- 向目的主机发送 IP 头中 DF 控制位为 1 的数据包，DF 是不分片（Don’t Fragment，DF）的缩写；
- 路径上的网络设备根据数据包的大小和自己的 MTU 做出不同的决定：
- 如果数据包大于设备的 MTU，就会丢弃数据包并发回一个包含该设备 MTU 的 ICMP 消息；
- 如果数据包小于设备的 MTU，就会继续向目的主机传递数据包；
- 源主机收到 ICMP 消息后，会不断使用新的 MTU 发送 IP 数据包，直到 IP 数据包达到目的主机；

ICMP 是互联网控制消息协议（Internet Control Message Protocol，ICMP），它能在 IP 主机之间传递控制消息。

TCP 协议是面向字节流的协议，应用层交给 TCP 协议的数据并不会以消息为单位向目的主机发送，应用层交给 TCP 协议发送的数据可能会被拆分到多个数据段中。

TCP 协议引入了最大分段大小（Maximum segment size，MSS）这一概念，它是 TCP 数据段能够携带的数据上限

IP 协议的 MTU 是物理设备上的限制，它限制了路径能够发送数据包的上限，而 TCP 协议的 MSS 是操作系统内核层面的限制，通信双方会在三次握手时确定这次连接的 MSS。一旦确定了 MSS，TCP 协议就会对应用层交给 TCP 协议发送的数据进行拆分，构成多个数据段。

需要注意的是，IP 协议和 TCP 协议虽然都会对数据进行拆分，但是 IP 协议以数据包（Package）为单位组织数据，而 TCP 协议以数据段（Segment）为单位组织数据。

数据拆分的根本原因说到底还是物理设备的限制，不过每一层协议都受限于下一层协议做出的决定，并依赖下层协议重新决定设计和实现的方法。虽然 TCP/IP 协议在传输数据时都需要对数据进行拆分，但是它们做出拆分数据的设计基于不同的上下文，也有着不同的目的，我们在这里总结一下两个网络协议做出类似决定的原因：

（1）IP 协议拆分数据是因为物理设备的限制，一次能够传输的数据由路径上 MTU 最小的设备决定，一旦 IP 协议传输的数据包超过 MTU 的限制就会发生丢包，所以我们需要通过路径 MTU 发现获取传输路径上的 MTU 限制；

（2）TCP 协议拆分数据是为了保证传输的可靠性和顺序，作为可靠的传输协议，为了保证数据的传输顺序，它需要为每一个数据段增加包含序列号的 TCP 协议头，如果数据段大小超过了 IP 协议的 MTU 限制， 就会带来更多额外的重传和重组开销，影响性能。

通过本文的分析，相信各位读者不仅了解了为什么 TCP/IP 协议会拆分数据，也了解了为什么 UDP 协议的数据报不应该超过 MTU - 28 字节，一旦超过该限制，IP 协议的分片机制会增加 UDP 数据报无法重组的可能性

### 018 为什么流媒体直播的延迟很高

（1）音视频使用的编码格式决定了客户端只能从特定帧开始解码；
（2）音视频传输使用的网络协议切片大小决定了客户端接收数据的间隔；
（3）服务器和客户端为了保证用户体验和直播质量预留缓存；

### 019 为什么 HTTPS 需要 7 次握手以及 9 倍时延

TCP 连接的双方会通过三次握手确定 TCP 连接的初始序列号、窗口大小以及最大数据段，这样通信双方就能利用连接中的初始序列号保证双方数据段的不重不漏、通过窗口大小控制流量并使用最大数据段避免 IP 协议对数据包的分片

（1）TCP 协议需要通过三次握手建立 TCP 连接保证通信的可靠性（1.5-RTT）；
（2）TLS 协议会在 TCP 协议之上通过四次握手建立 TLS 连接保证通信的安全性（2-RTT）；
（3）HTTP 协议会在 TCP 和 TLS 上通过一次往返发送请求并接收响应（1-RTT）；

### 020 为什么 TCP 协议有粘包问题

Nagle 算法是一种通过减少数据包的方式提高 TCP 传输性能的算法。因为网络 带宽有限，它不会将小的数据块直接发送到目的主机，而是会在本地缓冲区中等待更多待发送的数据，这种批量发送数据的策略虽然会影响实时性和网络延迟，但是能够降低网络拥堵的可能性并减少额外开销。

在早期的互联网中，Telnet 是被广泛使用的应用程序，然而使用 Telnet 会产生大量只有 1 字节负载的有效数据，每个数据包都会有 40 字节的额外开销，带宽的利用率只有 ~2.44%，Nagle 算法就是在当时的这种场景下设计的。

当应用层协议通过 TCP 协议传输数据时，实际上待发送的数据先被写入了 TCP 协议的缓冲区，如果用户开启了 Nagle 算法，那么 TCP 协议可能不会立刻发送写入的数据，它会等待缓冲区中数据超过最大数据段（MSS）或者上一个数据段被 ACK 时才会发送缓冲区中的数据。

除了 Nagle 算法之外，TCP 协议栈中还有另一个用于延迟发送数据的选项 TCP_CORK，如果我们开启该选项，那么当发送的数据小于 MSS 时，TCP 协议就会延迟 200ms 发送该数据或者等待缓冲区中的数据超过 MSS

既然 TCP 协议是基于字节流的，这其实就意味着应用层协议要自己划分消息的边界。

如果我们能在应用层协议中定义消息的边界，那么无论 TCP 协议如何对应用层协议的数据包进程拆分和重组，接收方都能根据协议的规则恢复对应的消息。在应用层协议中，最常见的两种解决方案就是基于长度或者基于终结符（Delimiter）。

TCP 协议粘包问题是因为应用层协议开发者的错误设计导致的，他们忽略了 TCP 协议数据传输的核心机制 — 基于字节流，其本身不包含消息、数据包等概念，所有数据的传输都是流式的，需要应用层协议自己设计消息的边界，即消息帧（Message Framing），我们重新回顾一下粘包问题出现的核心原因：

（1）TCP 协议是基于字节流的传输层协议，其中不存在消息和数据包的概念；
（2）应用层协议没有使用基于长度或者基于终结符的消息边界，导致多个消息的粘连；

UDP 的边界是怎么确定的？是根据长度确定边界吗？

每个 UDP 数据包都表示一个完整的消息

### 021 为什么 TCP 协议有 TIME_WAIT 状态

TIME_WAIT 仅在主动断开连接的一方出现，被动断开连接的一方会直接进入 CLOSED 状态，进入 TIME_WAIT 的客户端需要等待 2 MSL 才可以真正关闭连接。TCP 协议需要 TIME_WAIT 状态的原因和客户端需要等待两个 MSL 不能直接进入 CLOSED 状态的原因是一样的：

（1）防止延迟的数据段被其他使用相同源地址、源端口、目的地址以及目的端口的 TCP 连接收到；
（2）保证 TCP 连接的远程被正确关闭，即等待被动关闭连接的一方收到 FIN 对应的 ACK 消息；

RFC 793 中虽然指出了 TCP 连接需要在 TIME_WAIT 中等待 2 倍的 MSL（maximum segment lifetime），但是并没有解释清楚这里的两倍是从何而来，比较合理的解释是 — 网络中可能存在来自发起方的数据段，当这些发起方的数据段被服务端处理后又会向客户端发送响应，所以一来一回需要等待 2 倍的时间

TIME_WAIT 状态是 TCP 与不确定的网络延迟斗争的结果，而不确定性是 TCP 协议在保证可靠这条路的最大阻碍。

### 022 为什么 0.1 + 0.2 = 0.300000004

（1）二进制无法在有限地长度中精确地表示十进制中 0.1 和 0.2；
（2）单精度浮点数、双精度浮点数的位数决定了它们能够表示的精度上限；

当我们在不同编程语言中看到 0.300000004 或者 0.30000000000000004 时不应该感到惊讶，这其实说明编程语言正确实现了 IEEE 754 标准中描述的浮点数系统，在使用单精度和双精度浮点数时也应该牢记它们只有 7 位和 15 位的有效位数。

### 023 为什么 Linux 需要虚拟内存

虚拟内存是操作系统物理内存和进程之间的中间层，它为进程隐藏了物理内存这一概念，为进程提供了更加简洁和易用的接口以及更加复杂的功能。

操作系统以页为单位管理内存，当进程发现需要访问的数据不在内存时，操作系统可能会将数据以页的方式加载到内存中，这个过程是由上图中的内存管理单元（MMU）完成的。操作系统的虚拟内存作为一个抽象层，起到了以下三个非常关键的作用：

（1）虚拟内存可以利用磁盘起到缓存的作用，提高进程访问磁盘的速度；
（2）虚拟内存可以为进程提供独立的内存空间，简化程序的链接、加载过程并通过动态库共享内存；
（3）虚拟内存可以控制进程对物理内存的访问，隔离不同进程的访问权限，提高系统的安全性；

我们在 为什么 Redis 快照使用子进程 一文中介绍的写时复制就利用了虚拟内存的这个特性，当我们在 Linux 中调用 fork 创建子进程时，实际上只复制了父进程的页表。

### 024 为什么 MySQL 的自增主键不单调也不连续

在很多开发者的认知中，MySQL 的主键都应该是单调递增的，但是在我们与 MySQL 打交道的过程中会遇到两个问题，首先是记录的主键并不连续，其次是可能会创建多个主键相同的记录，我们将从以下的两个角度回答 MySQL 不单调和不连续的原因：

（1）较早版本的 MySQL 将 AUTO_INCREMENT 存储在内存中，实例重启后会根据表中的数据重新设置该值；
（2）获取 AUTO_INCREMENT 时不会使用事务锁，并发的插入事务可能出现部分字段冲突导致插入失败；

需要注意的是，我们在这篇文章中讨论的是 MySQL 中最常见的 InnoDB 存储引擎，MyISAM 等其他引擎提供的 AUTO_INCREMENT 实现原理不在本文的讨论范围中。

然而在 MySQL 8.0 中，AUTO_INCREMENT 计数器的初始化行为发生了改变，每次计数器的变化都会写入到系统的重做日志（Redo log）并在每个检查点存储在引擎私有的系统表中

当 MySQL 服务被重启或者处于崩溃恢复时，它可以从持久化的检查点和重做日志中恢复出最新的 AUTO_INCREMENT 计数器，避免出现不单调的主键的问题。

基于 MySQL 或者其他存储系统实现完全串行的插入也可以保证主键在插入时的连续，但是仍然不能避免删除数据导致的不连续。

### 025 为什么 0.1 + 0.2 = 0.3

使用十进制的两个整数 — 整数值和指数表示有限精度或者无限精度的小数，一些编程语言使用 128 位的 Decimal 表示具有 28 ~ 29 位精度的数字，而一些编程语言使用 BigDecimal 表示无限精度的数字；

使用十进制的两个整数 — 分子和分母表示准确的分数，可以减少浮点数计算带来的精度损失；

### 026 为什么 Mac 地址不需要全球唯一

在不同操作系统上，我们都可以通过软件直接修改网卡的 MAC 地址；
只需要保证一个局域网内的 MAC 地址不重复，网络就可以正常工作；

### 027 为什么 IPv6 难以取代 IPv4

NAT 技术可以很大程度上缓解 IPv4 的地址短缺问题并且能够保护私有内部的网络，提供防火墙的功能；
IPv4 与 IPv6 协议完全不兼容，我们需要引入双协议栈、隧道技术或者 NAT64 解决兼容性问题，而应用这些技术也需要额外的成本；
通过对资源的细粒度管控并回收不再使用的 IP 地址，可以延缓 IP 地址耗尽的时间；

### 028 为什么集群需要 Overlay 网络

因为 Overlay 网络是建立在另一个计算机网络之上的虚拟网络，所以它不能独立出现，Overlay 底层依赖的网络就是 Underlay 网络，这两个概念也经常成对出现。

Underlay 网络是专门用来承载用户 IP 流量的基础架构层，它与 Overlay 网络之间的关系有点类似物理机和虚拟机。Underlay 网络和物理机都是真正存在的实体，它们分别对应着真实存在的网络设备和计算设备，而 Overlay 网络和虚拟机都是依托在下层实体使用软件虚拟出来的层级。

Overlay 网络解决的三个问题：

（1）云计算中集群内的、跨集群的或者数据中心间的虚拟机和实例的迁移比较常见；
（2）单个集群中的虚拟机规模可能非常大，大量的 MAC 地址和 ARP 请求会为网络设备带来巨大的压力；
（3）传统的网络隔离技术 VLAN 只能建立 4096 个虚拟网络，公有云以及大规模的虚拟化集群需要更多的虚拟网络才能满足网络隔离的需求；

### 029 为什么系统调用会消耗较多资源

Linux 执行系统调用的三种方法：

（1）使用软件中断（Software interrupt）触发系统调用；
根据事件发出者的不同，我们可以将中断分成硬件和软件中断两种，硬件中断是由处理器外部的设备触发的电子信号；而软件中断是由处理器在执行特定指令时触发的，某些特殊的指令也可以故意触发软件中断

使用软件中断触发的系统调用需要保存堆栈和返回地址等信息，还要在中断描述表中查找系统调用的响应函数，虽然多数的操作系统不会使用 INT 0x80 触发系统调用，但是在一些特殊场景下，我们仍然需要利用这一古老的技术；

（2）使用 SYSCALL / SYSENTER 等汇编指令触发系统调用；
与 INT 0x80 通过触发软件中断实现系统调用不同，SYSENTER 和 SYSCALL 是专门为系统调用设计的汇编指令，它们不需要在中断描述表（Interrupt Descriptor Table、IDT）中查找系统调用对应的执行过程，也不需要保存堆栈和返回地址等信息，所以能够减少所需要的额外开销。

（3）使用虚拟动态共享对象（virtual dynamic shared object、vDSO）执行系统调用；
使用 vSDO 执行系统调用是操作系统为我们提供的最快路径，该方式可以将系统调用的开销与函数调用拉平，不过因为将内核态的系统调用映射到『用户态』确实存在安全风险，所以操作系统也仅会放开有限的系统调用；

### 030 为什么 Linux 默认页大小是 4KB

过小的页面大小会带来较大的页表项增加寻址时 TLB（Translation lookaside buffer）的查找速度和额外开销，但是也会减少程序中的内存碎片，提高内存的利用率；

过大的页面大小会浪费内存空间，造成内存碎片，降低内存的利用率，但是可以较少进程中的页表项以及 TLB 的寻址时间；

### 031 为什么数据库不应该使用外键

外键不仅仅是数据库表中的一个整数，它还提供了额外的一致性保证。因为数据库往往是整个系统的真理之源（Source of Truth），所以保证数据的一致性和正确性非常重要，关系型数据库虽然提供了外键、触发器等特性保证一致性，但是在今天的生产环境中却很少被使用。

（1）不使用外键牺牲了数据库中数据的一致性，但是却能够减少数据库的负载；
（2）模拟外键将一部分工作移到了数据库之外，我们可能需要放弃一部分一致性以获得更高的可用性，但是为了这部分可用性，我们会付出更多的研发与维护成本，也增加了与数据库之间的网络通信次数；
（3）使用外键保证了数据库中数据的一致性，也将全部的计算任务全部交给了数据库；
