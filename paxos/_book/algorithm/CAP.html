
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>1.1 CAP理论 · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="ZAB协议.html" />
    
    
    <link rel="prev" href="../" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    0 Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../">
            
                <a href="../">
            
                    
                    1 Algorithm
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter active" data-level="1.2.1" data-path="CAP.html">
            
                <a href="CAP.html">
            
                    
                    1.1 CAP理论
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="ZAB协议.html">
            
                <a href="ZAB协议.html">
            
                    
                    1.2 ZAB协议
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="分布式锁.html">
            
                <a href="分布式锁.html">
            
                    
                    1.3 分布式锁
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="raft-zh_cn.html">
            
                <a href="raft-zh_cn.html">
            
                    
                    1.4 RAFT算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="PAXOS.html">
            
                <a href="PAXOS.html">
            
                    
                    1.5 3分钟理解PAXOS算法
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../">
            
                <a href="../">
            
                    
                    2 Java
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="../java/从实际案例聊聊Java应用的GC优化.html">
            
                <a href="../java/从实际案例聊聊Java应用的GC优化.html">
            
                    
                    2.1 JavaGC优化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="../java/设计模式23.html">
            
                <a href="../java/设计模式23.html">
            
                    
                    2.2 设计模式
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="../java/Effective Java.html">
            
                <a href="../java/Effective Java.html">
            
                    
                    2.3 EffectiveJava
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4" data-path="../java/Java学习笔记.html">
            
                <a href="../java/Java学习笔记.html">
            
                    
                    2.4 Java学习笔记
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.5" data-path="../java/JDK相关.html">
            
                <a href="../java/JDK相关.html">
            
                    
                    2.5 JDK相关
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.6" data-path="../java/AlibabaJavaCodingGuidelines.html">
            
                <a href="../java/AlibabaJavaCodingGuidelines.html">
            
                    
                    2.6 阿里Java编码规范整理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.7" data-path="../java/Java程序执行次序.html">
            
                <a href="../java/Java程序执行次序.html">
            
                    
                    2.7 深入了解Java程序执行顺序
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.8" data-path="../java/JDK定时任务对比.html">
            
                <a href="../java/JDK定时任务对比.html">
            
                    
                    2.8 JDK定时任务对比
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../">
            
                <a href="../">
            
                    
                    3 SpringBoot
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="../springboot/SpringBoot-pom-xml.html">
            
                <a href="../springboot/SpringBoot-pom-xml.html">
            
                    
                    3.1 SpringBoot的pom.xml
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="../springboot/SpringBoot注解.html">
            
                <a href="../springboot/SpringBoot注解.html">
            
                    
                    3.2 SpringBoot的注解
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="../springboot/Eureka.html">
            
                <a href="../springboot/Eureka.html">
            
                    
                    3.3 Eureka
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="../springboot/LogBack相关.html">
            
                <a href="../springboot/LogBack相关.html">
            
                    
                    3.4 LogBack
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.5" data-path="../springboot/Maven相关.html">
            
                <a href="../springboot/Maven相关.html">
            
                    
                    3.5 Maven
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.6" data-path="../springboot/OAuth2.0.html">
            
                <a href="../springboot/OAuth2.0.html">
            
                    
                    3.6 OAuth2.0
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.7" data-path="../springboot/SpringBoot排错.html">
            
                <a href="../springboot/SpringBoot排错.html">
            
                    
                    3.7 SpringBoot排错
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.8" data-path="../springboot/SpringBoot排雷现场.html">
            
                <a href="../springboot/SpringBoot排雷现场.html">
            
                    
                    3.8 SpringBoot排雷现场
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.9" data-path="../springboot/SpringBoot释疑.html">
            
                <a href="../springboot/SpringBoot释疑.html">
            
                    
                    3.9 SpringBoot释疑
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.10" data-path="../springboot/SpringBoot引入log4j的配置.html">
            
                <a href="../springboot/SpringBoot引入log4j的配置.html">
            
                    
                    3.10 SpringBoot引入log4j的配置
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.11" data-path="../springboot/SpringBoot启动脚本.html">
            
                <a href="../springboot/SpringBoot启动脚本.html">
            
                    
                    3.11 SpringBoot启动脚本
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.12" data-path="../springboot/SpringBoot定时任务.html">
            
                <a href="../springboot/SpringBoot定时任务.html">
            
                    
                    3.12 SpringBoot定时任务
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.13" data-path="../springboot/SpringBoot初始化任务.html">
            
                <a href="../springboot/SpringBoot初始化任务.html">
            
                    
                    3.13 SpringBoot初始化任务
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.14" data-path="../springboot/SpringBootRedis.html">
            
                <a href="../springboot/SpringBootRedis.html">
            
                    
                    3.14 SpringBootRedis
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.15" data-path="../springboot/SpringBootRabbitMQ.html">
            
                <a href="../springboot/SpringBootRabbitMQ.html">
            
                    
                    3.15 SpringBootRabbitMQ
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.16" data-path="../springboot/packaging为pom标签的用法.html">
            
                <a href="../springboot/packaging为pom标签的用法.html">
            
                    
                    3.16 pom标签
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.17" data-path="../springboot/客户端配置更新.html">
            
                <a href="../springboot/客户端配置更新.html">
            
                    
                    3.17 客户端配置更新
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.18" data-path="../springboot/SpringBoot配置优先级.html">
            
                <a href="../springboot/SpringBoot配置优先级.html">
            
                    
                    3.18 SpringBoot配置优先级
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../">
            
                <a href="../">
            
                    
                    4 RabbitMQ
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="../rabbitmq/RabbitMQ相关.html">
            
                <a href="../rabbitmq/RabbitMQ相关.html">
            
                    
                    4.1 RabbitMQ相关
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="../rabbitmq/RabbitMQ四种发送方式区别.html">
            
                <a href="../rabbitmq/RabbitMQ四种发送方式区别.html">
            
                    
                    4.2 RabbitMQ四种发送方式区别
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.3" data-path="../rabbitmq/RabbitMQ安装全记录.html">
            
                <a href="../rabbitmq/RabbitMQ安装全记录.html">
            
                    
                    4.3 RabbitMQ安装全记录
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.4" data-path="../rabbitmq/RabbitMQ多地址连接.html">
            
                <a href="../rabbitmq/RabbitMQ多地址连接.html">
            
                    
                    4.4 RabbitMQ多地址连接
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../">
            
                <a href="../">
            
                    
                    5 Shell
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1" data-path="../shell/免密登陆.html">
            
                <a href="../shell/免密登陆.html">
            
                    
                    5.1 免密登陆
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.2" data-path="../shell/iptables命令.html">
            
                <a href="../shell/iptables命令.html">
            
                    
                    5.2 iptables命令
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.3" data-path="../shell/scp命令.html">
            
                <a href="../shell/scp命令.html">
            
                    
                    5.3 scp命令
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.4" data-path="../shell/shell特殊变量的含义.html">
            
                <a href="../shell/shell特殊变量的含义.html">
            
                    
                    5.4 shell特殊变量的含义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.5" data-path="../shell/TCP最大连接数.html">
            
                <a href="../shell/TCP最大连接数.html">
            
                    
                    5.5 TCP最大连接数
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../">
            
                <a href="../">
            
                    
                    6 ZooKeeper
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.7.1" data-path="../zookeeper/ZooKeeper集群安装.html">
            
                <a href="../zookeeper/ZooKeeper集群安装.html">
            
                    
                    6.1 ZooKeeper集群安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.2" data-path="../zookeeper/ZooKeeper权限控制.html">
            
                <a href="../zookeeper/ZooKeeper权限控制.html">
            
                    
                    6.2 ZooKeeper权限控制
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.3" data-path="../zookeeper/ZooKeeperWatcher.html">
            
                <a href="../zookeeper/ZooKeeperWatcher.html">
            
                    
                    6.3 ZooKeeperWatcher
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.4" data-path="../zookeeper/ZooKeeper连接.html">
            
                <a href="../zookeeper/ZooKeeper连接.html">
            
                    
                    6.4 ZooKeeper连接
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="../pic/PIC.html">
            
                <a href="../pic/PIC.html">
            
                    
                    7 Pic
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../">
            
                <a href="../">
            
                    
                    8 Other
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.9.1" data-path="../other/GitBook.html">
            
                <a href="../other/GitBook.html">
            
                    
                    1 GitBook
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.2" data-path="../other/MySQL.html">
            
                <a href="../other/MySQL.html">
            
                    
                    2 MySQL
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.3" data-path="../other/MD语法.html">
            
                <a href="../other/MD语法.html">
            
                    
                    3 MarkDown
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.4" data-path="../other/windows相关.html">
            
                <a href="../other/windows相关.html">
            
                    
                    4 Windows相关
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.5" data-path="../other/Sublime Text 3 快捷键精华版.html">
            
                <a href="../other/Sublime Text 3 快捷键精华版.html">
            
                    
                    5 SublimeText3快捷键精华版
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.6" data-path="../other/zabbix.html">
            
                <a href="../other/zabbix.html">
            
                    
                    6 Zabbix
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.7" data-path="../other/Nginx安装.html">
            
                <a href="../other/Nginx安装.html">
            
                    
                    7 Nginx
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.8" data-path="../other/TBSchedule.html">
            
                <a href="../other/TBSchedule.html">
            
                    
                    8 TBSchedule
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.9" data-path="../other/webdis安装.html">
            
                <a href="../other/webdis安装.html">
            
                    
                    9 webdis安装
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >1.1 CAP理论</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="perspectives-on-the-cap-theorem">Perspectives on the CAP Theorem</h1>
<pre><code>Seth Gilbert          National University of Singapore
Nancy A. Lynch        Massachusetts Institute of Technology
</code></pre><h1 id="abstract">Abstract</h1>
<p>Almost twelve years ago, in 2000, Eric Brewer introduced the idea that there is a fundamental trade-off between consistency, availability, and partition tolerance. This trade-off, which has become known as the CAP Theorem, has been widely discussed ever since. In this paper, we review the CAP Theorem and situate it within the broader context of distributed computing theory. We then discuss the practical implications of the CAP Theorem, and explore some general techniques for coping with the inherent trade-offs that it implies.</p>
<p>C-&#x4E00;&#x81F4;&#x6027;&#xFF0C;A-&#x53EF;&#x7528;&#x6027;&#xFF0C;P-&#x5206;&#x79BB;&#x5BB9;&#x9519;&#x6027;&#x7684;&#x6743;&#x8861;&#x3002;&#x672C;&#x6587;&#x8BA8;&#x8BBA;CAP&#x7406;&#x8BBA;&#x5728;&#x5DE5;&#x7A0B;&#x4E2D;&#x7684;&#x5B9E;&#x73B0;&#x3002;</p>
<h1 id="1-introduction">1 Introduction</h1>
<p>Almost twelve years ago, in 2000, Eric Brewer introduced the idea that there is a fundamental trade-off between consistency, availability, and partition tolerance. This trade-off, which has become known as the CAP Theorem, has been widely discussed ever since.</p>
<h2 id="theoretical-context">Theoretical context.</h2>
<p>Our first goal in this paper is to situate the CAP Theorem in the broader context of distributed computing theory. Some of the interest in the CAP Theorem, perhaps, derives from the fact that it illustrates a more general trade-off that appears everywhere (e.g., [4,7,15,17,23]) in the study of distributed computing: the impossibility of guaranteeing both safety and liveness in an unreliable distributed system:</p>
<p>&#x5728;&#x4E0D;&#x53EF;&#x9760;&#x7684;&#x5206;&#x5E03;&#x5F0F;&#x7CFB;&#x7EDF;&#x4E2D;&#xFF0C;&#x4E0D;&#x53EF;&#x80FD;&#x540C;&#x65F6;&#x6EE1;&#x8DB3;&#x5B89;&#x5168;&#x6027;&#x548C;&#x6D3B;&#x6027;&#x3002;</p>
<p><strong>Safety</strong>: Informally, an algorithm is safe if nothing bad ever happens. A quiet, uneventful room is perfectly safe. Consistency (as defined in the CAP Theorem) is a classic safety property: every response sent to a client is correct.</p>
<p>&#x5B89;&#x5168;&#x6027;&#xFF1A;&#x574F;&#x7684;&#x4E8B;&#x60C5;&#x4E00;&#x5B9A;&#x4E0D;&#x4F1A;&#x53D1;&#x751F;&#x3002;&#x4E00;&#x81F4;&#x6027;&#x662F;&#x4E00;&#x79CD;&#x5B89;&#x5168;&#x6027;&#x3002;</p>
<p><strong><em>Liveness</em></strong>: By contrast, an algorithm is live if eventually something good happens. In a busy pub, there may be some good things happening, and there may be some bad things happening&#x2014;but overall, it is quite lively. Availability is a classic liveness property: eventually, every request receives a response.</p>
<p>&#x6D3B;&#x6027;&#xFF1A;&#x597D;&#x7684;&#x4E8B;&#x60C5;&#x603B;&#x4F1A;&#x53D1;&#x751F;&#x3002;&#x53EF;&#x7528;&#x6027;&#x662F;&#x4E00;&#x79CD;&#x6D3B;&#x6027;&#x3002;</p>
<p><strong><em>Unreliable</em></strong>: There are many different ways in which a system can be unreliable. There may be partitions, as is discussed in the CAP Theorem. Alternatively, there may be crash failures, message loss, malicious attacks (or Byzantine failures), etc.</p>
<p>&#x4E0D;&#x53EF;&#x9760;&#xFF1A;&#x5B95;&#x673A;&#x3001;&#x4E22;&#x5305;&#x7B49;&#x3002;</p>
<p>The CAP Theorem, in this light, is simply one example of the fundamental fact that you cannot achieve both safety and liveness in an unreliable distributed system.</p>
<p>CAP&#x7406;&#x8BBA;&#x4E5F;&#x5728;&#x201C;&#x5728;&#x4E0D;&#x53EF;&#x9760;&#x7684;&#x5206;&#x5E03;&#x5F0F;&#x7CFB;&#x7EDF;&#x4E2D;&#xFF0C;&#x4E0D;&#x53EF;&#x80FD;&#x540C;&#x65F6;&#x6EE1;&#x8DB3;&#x5B89;&#x5168;&#x6027;&#x548C;&#x6D3B;&#x6027;&#x201D;&#x7684;&#x6846;&#x67B6;&#x4E4B;&#x5185;&#x3002;</p>
<h3 id="practical-implications">Practical implications.</h3>
<p>Our second goal in this paper is to discuss some of the practical implications of the CAP Theorem. Since it is impossible to achieve both consistency and availability in an unreliable system, it is necessary in practice to sacrifice one of these two desired properties. On the one hand, there are systems that guarantee strong consistency and provide best effort availability. On the other hand, there are systems that guarantee availability, and provide best effort consistency. Perhaps surprisingly, there is a third option: some systems may sacrifice both consistency and availability! In doing so they may achieve a trade-off better suited for the application at hand.</p>
<p>&#x8BF4;&#x5230;&#x5E95;&#xFF0C;&#x8981;&#x6839;&#x636E;&#x5177;&#x4F53;&#x60C5;&#x51B5;&#x6743;&#x8861;&#x5B89;&#x5168;&#x6027;&#x4E0E;&#x6D3B;&#x6027;&#x3002;</p>
<h3 id="roadmap">Roadmap.</h3>
<p>We begin in Section 2 by reviewing the CAP Theorem, as it was formalized in [16]. We then examine in Section 3 how the CAP Theorem fits into the general framework of a trade-off between safety and liveness. Finally, in Section 4, we discuss the implications of this trade-off, and various strategies for coping with it. We conclude in Section 5 with a short discussion of some new directions to consider in the context of the CAP Theorem.</p>
<h1 id="2-the-cap-theorem">2 The CAP Theorem</h1>
<p>We begin, in this section, by reviewing Brewer&#x2019;s original conjecture (or, more specifically, one interpretation of the original conjecture). We discuss how the conjecture can be proved, closely following the presentation in [16]. </p>
<p>Brewer first presented the CAP Theorem in the context of a web service. A web service is implemented by a set of servers, perhaps distributed over a set of geographically distant data centers. Clients make requests of the service. When a server receives a request from the service, it sends a response. Notice that such a generic notion of a web service can capture a wide variety of applications, such as search engines, e-commerce, on-line music services, or cloud-based data storage. For the purpose of this discussion, we will imagine the service to consist of servers p1, p2, . . . , pn, along with an arbitrary set of clients.</p>
<p>The CAP Theorem was introduced as a trade-off between consistency, availability, and partition tolerance. We now discuss each of these terms.</p>
<h2 id="consistency">Consistency.</h2>
<p>Consistency, informally, simply means that each server returns the right response to each request, i.e., a response that is correct according to the desired service specification. (There may, of course, be multiple possible correct responses.) The meaning of consistency depends on the service.</p>
<p><strong><em>Trivial services</em></strong>: Some services are trivial in the sense that they do not require any coordination among the servers. For example, if the service is supposed to return the value of the constant &#x3C0; to 100 decimal places, then a correct response is exactly that. Since no coordination among the servers is required, trivial services do not fall within the scope of the CAP Theorem.</p>
<p>&#x670D;&#x52A1;&#x63D0;&#x4F9B;&#x8005;&#x4E4B;&#x95F4;&#x6CA1;&#x6709;&#x4EA4;&#x4E92;&#x7684;&#xFF0C;&#x4E0D;&#x5728;CAP&#x7684;&#x8BA8;&#x8BBA;&#x4E4B;&#x5217;&#x3002;</p>
<p><strong><em>Weakly consistent services</em></strong>: In response to the inherent trade-offs implied by the CAP Theorem, there has been much work attempting to develop weaker consistency requirements that still provide useful services and yet avoid sacrificing availability. A distributed web cache is an example of one such system. We will discuss such systems later in Section 4.2.</p>
<p><strong><em>Simple services</em></strong>: For the purposes of discussing the CAP Theorem, we focus on simple services that have straightforward correctness requirements: the semantics of the service are specified by a sequential specification, and operations are atomic. A sequential specification defines a service in terms of its execution on a single, centralized server: the centralized server maintains some state, and each request is processed in order, updating the state and generating a response. A web service is atomic if, for every operation, there is a single instant in between the request and the response at which the operation appears to occur. (Alternatively, this is equivalent to saying that, from the perspective of the clients, it is as if all the operations were executed by a single centralized server.) While there are several weaker consistency conditions used in practice (e.g., sequential consistency, causal consistency, etc.), we focus on atomicity due to its simplicity.</p>
<p>&#x539F;&#x5B50;&#x6027;&#xFF1A;&#x64CD;&#x4F5C;&#x7684;&#x8C03;&#x7528;&#x548C;&#x8FD4;&#x56DE;&#x4E4B;&#x95F4;&#x6709;&#x4E00;&#x4E2A;&#x65F6;&#x95F4;&#x6BB5;&#xFF0C;&#x64CD;&#x4F5C;&#x53EF;&#x88AB;&#x89C6;&#x4E3A;&#x5728;&#x8FD9;&#x6BB5;&#x65F6;&#x95F4;&#x7684;&#x67D0;&#x4E2A;&#x65F6;&#x523B;&#x77AC;&#x95F4;&#x5B8C;&#x6210;&#xFF0C;&#x64CD;&#x4F5C;&#x6240;&#x5E26;&#x6765;&#x7684;&#x526F;&#x4F5C;&#x7528;&#x5728;&#x90A3;&#x4E2A;&#x65F6;&#x523B;&#x77AC;&#x95F4;&#x663E;&#x73B0;&#x3002;&#x5EFA;&#x8BAE;&#x770B;&#x53EF;&#x7EBF;&#x6027;&#x5316;(Linearizability)&#x6765;&#x7406;&#x89E3;&#x3002;</p>
<p>&#x4E00;&#x81F4;&#x6027;&#x9664;&#x4E86;&#x5F3A;&#x5F31;&#x4E4B;&#x5206;&#xFF0C;&#x8FD8;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x79CD;&#x7C7B;&#xFF1A;&#x9759;&#x6001;&#x4E00;&#x81F4;&#x6027;&#xFF08;quiescent consistency&#xFF09;&#xFF0C;&#x987A;&#x5E8F;&#x4E00;&#x81F4;&#x6027;&#xFF08;sequential consistency&#xFF09;&#xFF0C;&#x53EF;&#x4E32;&#x6027;&#x5316;&#xFF08;Serializability&#xFF09;&#xFF0C;&#x53EF;&#x7EBF;&#x6027;&#x5316;(Linearizability)&#x3002;</p>
<p><strong><em>Complicated services</em></strong>: Many real services have more complicated semantics. Some services cannot be specified by sequential specifications. Others simply require more complicated coordination, transactional semantics, etc. The same CAP trade-offs typically apply, but for simplicity we do not focus on these cases.</p>
<p>For the purpose of this section, we focus on a service that implements a read/write atomic shared memory: the service provides its clients with a single (emulated) register, and each client can read or write from that register.</p>
<p>&#x8BA8;&#x8BBA;&#x7684;&#x6A21;&#x578B;&#x7B80;&#x5316;&#x4E3A;&#xFF1A;&#x670D;&#x52A1;&#x63D0;&#x4F9B;&#x539F;&#x5B50;&#x5BC4;&#x5B58;&#x5668;&#x4F9B;&#x5BA2;&#x6237;&#x7AEF;&#x8BFB;&#x5199;&#x3002;</p>
<h2 id="availability">Availability.</h2>
<p>The second requirement of the CAP Theorem is that the service guarantee availability. Availability simply means that each request eventually receive a response. Obviously, a fast response is better than a slow response, but for the purpose of CAP, it turns out that even requiring an eventual response is sufficient to create problems. (In most real systems, of course, a response that is sufficiently late is just as bad as a response that never occurs.)</p>
<p>&#x53EF;&#x7528;&#x6027;&#xFF1A;&#x8C03;&#x7528;&#x603B;&#x4F1A;&#x6709;&#x8FD4;&#x56DE;&#xFF08;&#x6D3B;&#x6027;&#xFF09;&#x3002;</p>
<h2 id="partition-tolerance">Partition-tolerance.</h2>
<p>The third requirement of the CAP theorem is that the service be partition tolerant. Unlike the other two requirements, this property can be seen as a statement regarding the underlying system: communication among the servers is not reliable, and the servers may be partitioned into multiple groups that cannot communicate with each other. For our purposes, we simply treat communication as faulty: messages may be delayed and, sometimes, lost forever. (Again, it is worth pointing out that a message that is delayed for sufficiently long may as well be considered lost, at least in the context of practical systems.)</p>
<p>&#x5BB9;&#x9519;&#x6027;&#xFF1A;&#x5206;&#x5E03;&#x5F0F;&#x7CFB;&#x7EDF;&#x7684;&#x4E00;&#x79CD;&#x72B6;&#x6001;&#xFF0C;&#x670D;&#x52A1;&#x4E4B;&#x95F4;&#x7684;&#x901A;&#x4FE1;&#x4E0D;&#x53EF;&#x9760;&#x3002;</p>
<h2 id="the-cap-theorem">The CAP Theorem.</h2>
<p>Thus, for the purpose of this section, the CAP Theorem can be stated as follows: In a network subject to communication failures, it is impossible for any web service to implement an atomic read/write shared memory that guarantees a response to every request.</p>
<h2 id="proof-sketch">Proof sketch.</h2>
<p>Having stated the CAP Theorem, it is relatively straightforward to prove it correct. (See [16] for more details.) Consider an execution in which the servers are partitioned into two disjoint sets: {p1} and {p2, . . . , pn}.</p>
<p>Some client sends a read request to server p2. Since p1 is in a different component of the partition from p2, every message from p1 to p2 is lost. Thus, it is impossible for p2 to distinguish the following two cases:</p>
<p>&#x2022; There has been a previous write of value v1 requested of p1, and p1 has sent an ok response.</p>
<p>&#x2022; There has been a previous write of value v2 requested of p1, and p1 has sent an ok response.</p>
<p>No matter how long p2 waits, it cannot distinguish these two cases, and hence it cannot determine whether to return response v1 or response v2. It has the choice to either eventually return a response (and risk returning the wrong response) or to never return a response.</p>
<p>In fact, if communication is asynchronous (i.e., processes have no a priori bound on how long it takes for a message to be delivered), it is possible for the same situation to occur even in executions in which there are no messages lost, i.e., no actual partition! Notably, in the scenario describe above, server p2 eventually must return a response, even if the system is partitioned; if the message delay from p1 to p2 is sufficiently large that p2 believes the system to be partitioned, then it may return an incorrect response, despite the lack of partitions. Thus it is even impossible to guarantee consistency when there are no partitions, and return a bad (inconsistent) answer only when partitions occur.</p>
<p>&#x56E0;&#x4E3A;&#x5206;&#x79BB;&#xFF0C;&#x53EF;&#x7528;&#x7684;&#x670D;&#x52A1;&#x65E0;&#x6CD5;&#x533A;&#x5206;&#x8C03;&#x7528;&#x5230;&#x5E95;&#x662F;&#x4EC0;&#x4E48;&#xFF08;&#x4F8B;&#x5B50;&#x4E2D;&#x662F;&#x4E24;&#x4E2A;&#x4E0D;&#x540C;&#x7684;&#x5199;&#x64CD;&#x4F5C;&#xFF09;&#x3002;&#x90A3;&#x4E48;&#x6709;&#x4E24;&#x4E2A;&#x9009;&#x62E9;&#xFF1A;&#x5176;&#x4E00;&#xFF0C;&#x7EE7;&#x7EED;&#x7B49;&#x4E0B;&#x53BB;&#xFF0C;&#x76F4;&#x5230;&#x660E;&#x786E;&#x8C03;&#x7528;&#xFF0C;&#x8FD9;&#x6837;&#x8FDD;&#x53CD;&#x53EF;&#x7528;&#x6027;&#xFF1B;&#x5176;&#x4E8C;&#xFF0C;&#x4EFB;&#x610F;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x503C;&#xFF0C;&#x8FD9;&#x6837;&#x53EF;&#x80FD;&#x8FDD;&#x53CD;&#x4E00;&#x81F4;&#x6027;&#x3002;</p>
<p>&#x5982;&#x679C;&#x901A;&#x4FE1;&#x662F;&#x5F02;&#x6B65;&#x7684;&#xFF08;&#x901A;&#x4FE1;&#x65F6;&#x95F4;&#x957F;&#x77ED;&#x4E0D;&#x5B9A;&#xFF09;&#xFF0C;&#x5373;&#x4F7F;&#x6CA1;&#x6709;&#x5206;&#x79BB;&#xFF08;&#x670D;&#x52A1;&#x5206;&#x6210;&#x51E0;&#x4E2A;&#x90E8;&#x5206;&#xFF0C;&#x4E4B;&#x95F4;&#x4E0D;&#x80FD;&#x901A;&#x4FE1;&#xFF09;&#xFF0C;&#x4E5F;&#x4F1A;&#x51FA;&#x73B0;&#x4E0A;&#x9762;&#x7684;&#x60C5;&#x51B5;&#x3002;</p>
<h1 id="3-theoretical-context">3 Theoretical Context</h1>
<p>The trade-off between consistency and availability in a partition-prone system is a particular example of the more general trade-off between safety and liveness in an unreliable system. This impossibility has played a key role in the study of distributed computing, and perhaps provides some insight into both the CAP Theorem and the manner in which algorithm designers and software engineers have circumvented it.</p>
<h2 id="31-consistent-availability-and-partition-tolerance">3.1 Consistent, Availability, and Partition-Tolerance</h2>
<p>We first review what it means for a property to be a &#x201C;safety property&#x201D; or a &#x201C;liveness property,&#x201D; indicating their connection
to the CAP Theorem.</p>
<p>A safety property is one that states nothing bad ever happens. That is, it requires that at every point in every execution, the property holds. Consistency requirements are almost always safety properties. For example, when we say that an algorithm guarantees atomic consistency, we are claiming that in every execution, every response is correct(with respect to the other &#x201C;prior&#x201D; operations).</p>
<p>A liveness property is one that states that eventually something good happens. A liveness property says nothing about the state at any instant in time; it requires only that if an execution continues for long enough, then something desirable happens. Availability is a classic liveness property: it states that eventually, every request receives a response.</p>
<p>Thus, from this perspective, the CAP Theorem states that it is impossible for any protocol implementing an atomic read/write register to guarantee both safety and liveness in a system prone to partitions.</p>
<h2 id="32-agreement-is-impossible">3.2 Agreement is Impossible</h2>
<p>Understanding the relationship between safety and liveness properties has been a long-standing question in distributed computing. It achieved widespread prominence when, in 1985, Fischer, Lynch, and Paterson [15] showed that fault-tolerant agreement is impossible in an asynchronous system. </p>
<p>&#x4E0D;&#x53EF;&#x80FD;&#x7ED3;&#x8BBA;&#xFF1A;fault-tolerant agreement is impossible in an asynchronous system.</p>
<p>&#x4E00;&#x81F4;&#x6027;&#x4E0D;&#x53EF;&#x80FD;&#x7ED3;&#x8BBA;&#xFF1A;&#x5728;&#x5F02;&#x6B65;&#x7CFB;&#x7EDF;&#x4E2D;&#xFF0C;&#x8FBE;&#x6210;&#x4E00;&#x81F4;&#x662F;&#x4E0D;&#x53EF;&#x80FD;&#x7684;&#x3002;</p>
<p>They focused on the problem of consensus, in which each process pi begins with an initial value vi, and the processes all have to agree on one of those values. There are three requirements: (i) agreement: every process must output the same value; (ii) validity: every value output must have been provided as the input for some process; and (iii) termination: eventually, every process must output a value. It should be immediately clear that agreement and validity are safety properties, while termination is a liveness property, and hence the impossibility of consensus is an important example of the inherent trade-off between safety and liveness.</p>
<p>&#x8FD9;&#x91CC;&#x8BF4;&#x7684;&#x662F;&#x4E00;&#x81F4;&#x6027;&#x534F;&#x8BAE;&#xFF08;consensus protocol&#xFF09;&#x3002;&#x573A;&#x666F;&#x662F;&#xFF1A;&#x6709;&#x4E00;&#x5806;&#x8FDB;&#x7A0B;&#x60F3;&#x8981;&#x5BF9;&#x67D0;&#x4E2A;&#x516C;&#x6709;&#x7684;&#x53D8;&#x91CF;&#x8FDB;&#x884C;&#x521D;&#x59CB;&#x5316;&#xFF0C;&#x6BCF;&#x4E2A;&#x8FDB;&#x7A0B;&#x90FD;&#x60F3;&#x521D;&#x59CB;&#x5316;&#x4E3A;&#x81EA;&#x5DF1;&#x7684;&#x503C;&#x3002;&#x4E00;&#x81F4;&#x6027;&#x534F;&#x8BAE;&#x5FC5;&#x987B;&#x6EE1;&#x8DB3;&#x4E09;&#x70B9;&#xFF1A;</p>
<p>&#x4E00;&#x81F4;&#xFF1A;&#x6240;&#x6709;&#x7684;&#x8FDB;&#x7A0B;&#x5BF9;&#x67D0;&#x4E2A;&#x503C;&#x8FBE;&#x6210;&#x4E00;&#x81F4;&#xFF1B;</p>
<p>&#x5408;&#x6CD5;&#xFF1A;&#x8FBE;&#x6210;&#x4E00;&#x81F4;&#x7684;&#x503C;&#x80AF;&#x5B9A;&#x662F;&#x67D0;&#x4E2A;&#x8FDB;&#x7A0B;&#x7684;&#x503C;&#xFF08;&#x4E0D;&#x662F;&#x51ED;&#x7A7A;&#x51FA;&#x73B0;&#x7684;&#x503C;&#xFF09;&#xFF1B;</p>
<p>&#x7EC8;&#x6B62;&#xFF1A;&#x6700;&#x7EC8;&#x53D8;&#x91CF;&#x80AF;&#x5B9A;&#x4F1A;&#x88AB;&#x521D;&#x59CB;&#x5316;&#xFF08;&#x4E0D;&#x53EF;&#x80FD;&#x8FD0;&#x884C;&#x65E0;&#x9650;&#x65F6;&#x95F4;&#xFF09;&#x3002;</p>
<p>The problem of consensus has attracted significant interest, as it is at the heart of the replicated state machine approach [8,19&#x2013;21], one of the most common techniques for building reliable distributed systems. In order to improve availability, a service may be replicated at a set of servers. In order to maintain consistency, the servers agree on every update to the service&#x2014;and notably, on the order of the updates.</p>
<p>The safety requirements of consensus are strictly harder than those we have considered with respect to the CAP Theorem: achieving agreement is (provably) harder than simply implementing an atomic read/write register. This means that the CAP Theorem also implies that you cannot achieve consensus in a system subject to partitions.</p>
<p>In [15], Fischer et al. considered a system with no partitions. Instead, the focus on an even more benign failure model: crash failures. Specifically, they assume that one (unknown) process in the system may fail by crashing, i.e., it may cease operation. No partitions are possible, and almost all of the processes in the system can continue to communicate reliably.</p>
<p>The surprising conclusion of [15] is that in such a system, consensus is impossible. In fact, for every purported consensus protocol that guarantees agreement and validity, there is some execution in which there are no failures where the algorithm never terminates! In the case of consensus, safety and liveness are impossible if the system is even potentially slightly faulty.</p>
<p>The surprising conclusion&#xFF1A;safety and liveness are impossible if the system is even potentially slightly faulty</p>
<p>&#x5206;&#x5E03;&#x5F0F;&#x7CFB;&#x7EDF;&#x4E2D;&#xFF0C;&#x5373;&#x4F7F;&#x4E0D;&#x5B58;&#x5728;&#x5206;&#x79BB;&#xFF08;&#x670D;&#x52A1;&#x5206;&#x6210;&#x51E0;&#x4E2A;&#x90E8;&#x5206;&#xFF0C;&#x4E4B;&#x95F4;&#x4E0D;&#x80FD;&#x901A;&#x4FE1;&#xFF09;&#xFF0C;&#x800C;&#x53EA;&#x662F;&#x4E00;&#x4E2A;&#x8FDB;&#x7A0B;&#x6302;&#x6389;&#x4E86;&#xFF08;&#x610F;&#x5473;&#x7740;&#x64CD;&#x4F5C;&#x505C;&#x6B62;&#xFF09;&#xFF0C;&#x5176;&#x4ED6;&#x8FDB;&#x7A0B;&#x90FD;&#x80FD;&#x53EF;&#x9760;&#x7684;&#x901A;&#x4FE1;&#xFF08;&#x5F88;&#x5F3A;&#x7684;&#x5047;&#x8BBE;&#x4E86;&#xFF09;&#xFF0C;&#x8FBE;&#x6210;&#x4E00;&#x81F4;&#xFF08;consensus&#xFF09;&#x4E5F;&#x662F;&#x4E0D;&#x53EF;&#x80FD;&#x7684;&#xFF08;&#x8FD9;&#x4E2A;&#x7ED3;&#x8BBA;&#x662F;&#x4E0D;&#x662F;&#x8BA9;&#x4F60;&#x611F;&#x89C9;&#x8FDD;&#x53CD;&#x76F4;&#x89C9;&#xFF1F;&#xFF09;</p>
<h2 id="33-coping-with-the-safetyliveness-trade-off-for-consensus">3.3 Coping with the Safety/Liveness Trade-off for Consensus</h2>
<p>Almost immediately after the publication of [15], researchers in distributed computing began examining this inherent rade-off between safety and liveness in more depth. While the results discussed in this section focus on the problem of consensus, they provide context for the analogous questions raised by the CAP Theorem.</p>
<p>Given that safety and liveness are impossible in systems that are sufficiently unreliable, the first natural question was under what conditions it is possible to achieve both. Much research has focused on the issue of network synchrony: what level of synchrony is necessary to avoid the inherent trade-off? How many failures can be tolerated? In the language of the CAP Theorem: what level of network reliability is needed to achieve both consistency and availability? In Section 3.3.1, we briefly review the connection between network synchrony and the (im)possibility of achieving consensus.</p>
<p>&#x4E00;&#x79CD;&#x60F3;&#x6CD5;&#x662F;&#xFF0C;&#x600E;&#x6837;&#x624D;&#x80FD;&#x540C;&#x65F6;&#x6EE1;&#x8DB3;&#x5B89;&#x5168;&#x6027;&#x4E0E;&#x6D3B;&#x6027;&#xFF1F;</p>
<p>&#x5BF9;&#x5E94;CAP&#x7684;&#x95EE;&#x9898;&#x662F;&#xFF1A;&#x7F51;&#x7EDC;&#x8FBE;&#x5230;&#x600E;&#x6837;&#x7684;&#x53EF;&#x9760;&#x6027;&#x624D;&#x80FD;&#x6EE1;&#x8DB3;&#x4E00;&#x81F4;&#x6027;&#x4E0E;&#x53EF;&#x7528;&#x6027;&#xFF1F;</p>
<p>A second natural question focuses on the question of consistency: given that the network is unreliable, what is the maximum level of consistency that can be achieved? In Section 3.3.2 we briefly review one answer to this question.</p>
<p>&#x53E6;&#x4E00;&#x79CD;&#x60F3;&#x6CD5;&#x662F;&#xFF0C;&#x5728;&#x4E0D;&#x53EF;&#x9760;&#x7684;&#x7F51;&#x7EDC;&#x4F20;&#x8F93;&#x4E2D;&#xFF0C;&#x80FD;&#x4FDD;&#x8BC1;&#x591A;&#x5F3A;&#x7684;&#x4E00;&#x81F4;&#x6027;&#xFF1F;</p>
<h3 id="331-synchrony">3.3.1 Synchrony</h3>
<p>In an attempt to answer the question of how much reliability is needed to solve consensus (i.e., to satisfy both the safety and liveness properties), researchers have focused on the issue of network synchrony.</p>
<p>A network is synchronous if it satisfies the following properties: (i) every process has a clock, and all the clocks are synchronized; (ii) every message is delivered within a fixed and known amount of time; and (iii) every process takes steps at a fixed and known rate. We can think of such systems as progressing in rounds, where within each round, each process: sends some messages, receives all the messages that were sent to it in that round, and performs some local computation.</p>
<p>&#x540C;&#x6B65;&#x7F51;&#x7EDC;&#x6EE1;&#x8DB3;&#xFF1A;&#x6BCF;&#x4E2A;&#x8FDB;&#x7A0B;&#x90FD;&#x6709;&#x4E00;&#x4E2A;&#x540C;&#x6B65;&#x7684;&#x65F6;&#x949F;&#xFF1B;&#x6D88;&#x606F;&#x4F20;&#x9012;&#x7684;&#x65F6;&#x95F4;&#x662F;&#x6709;&#x754C;&#xFF1B;&#x8FDB;&#x7A0B;&#x7684;&#x64CD;&#x4F5C;&#x6B65;&#x9AA4;&#x90FD;&#x662F;&#x6709;&#x754C;&#x7684;&#x3002;</p>
<p><strong><em>Time complexity</em></strong>. If a system is wholly synchronous, consensus can be solved, i.e., the trade-off between safety and liveness can be avoided. Notably, consensus requires f + 1 rounds, if up to f servers may crash (see [18, 22]). Since consensus is impossible in an asynchronous system if there is even one crash failure, this motivates the question: how much synchrony is needed to solve consensus? And do real systems provide that necessary level of synchrony?</p>
<p>&#x5982;&#x679C;&#x4E00;&#x4E2A;&#x7CFB;&#x7EDF;&#x662F;&#x5B8C;&#x5168;&#x540C;&#x6B65;&#x7684;&#xFF0C;&#x5219;&#x53EF;&#x8FBE;&#x6210;&#x4E00;&#x81F4;&#xFF08;consensus&#xFF09;&#x3002;&#x610F;&#x5473;&#x7740;&#x5728;F&#x4E2A;&#x670D;&#x52A1;&#x53EF;&#x80FD;&#x5B95;&#x673A;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x53EA;&#x8981; F+1 &#x8F6E;&#x5C31;&#x80FD;&#x8FBE;&#x6210;&#x4E00;&#x81F4;&#xFF08;consensus&#xFF09;&#x3002;&#x8FD9;&#x5C31;&#x5E26;&#x6765;&#x4E00;&#x4E2A;&#x95EE;&#x9898;&#xFF1A;&#x8981;&#x6709;&#x591A;&#x540C;&#x6B65;&#x624D;&#x80FD;&#x89E3;&#x51B3;&#x4E00;&#x81F4;&#x6027;&#xFF08;consensus&#xFF09;&#x95EE;&#x9898;&#xFF1F;</p>
<p>Soon after [15], Dwork et al. [14] attempted to answer this question, exploring a variety of models of partial synchrony. Most notably, they introduced the idea of eventual synchrony: a system may experience some periods of synchrony and some periods of asynchrony, but as long as it eventually stabilizes and maintains synchrony for a sufficiently long period of time, we can solve consensus.</p>
<p>&#x6700;&#x7EC8;&#x540C;&#x6B65;&#xFF1A;&#x4E00;&#x4E2A;&#x7CFB;&#x7EDF;&#x603B;&#x5728;&#x540C;&#x6B65;&#x4E0E;&#x5F02;&#x6B65;&#x95F4;&#x6765;&#x56DE;&#x4EA4;&#x66FF;&#xFF0C;&#x4F46;&#x53EA;&#x8981;&#x5176;&#x5728;&#x4E00;&#x6BB5;&#x8DB3;&#x591F;&#x7684;&#x65F6;&#x95F4;&#x5185;&#x4FDD;&#x6301;&#x540C;&#x6B65;&#xFF0C;&#x5219;&#x53EF;&#x4EE5;&#x89E3;&#x51B3;&#x4E00;&#x81F4;&#x6027;&#xFF08;consensus&#xFF09;&#x95EE;&#x9898;&#x3002;</p>
<p>How long a &#x201C;window of synchrony&#x201D; is necessary to solve consensus? Dutta and Guerraoui [13] showed that at least
f + 2 rounds are necessary. Alistarh et al. [3] recently resolved the question, showing that f + 2 rounds of synchrony are also sufficient.</p>
<p>&#x90A3;&#x4E48;&#x8981;&#x591A;&#x957F;&#x65F6;&#x95F4;&#x624D;&#x7B97;&#x8DB3;&#x591F;&#x5462;&#xFF1F; &#x53EA;&#x8981; F+2 &#x8F6E;&#x7684;&#x65F6;&#x95F4;&#x5C31;&#x591F;&#x4E86;&#x3002;</p>
<p>There is also a connection between the synchrony of a system and the crash-tolerance. In a synchronous system, we can solve consensus for any number of failures. In an asynchronous system, consensus is impossible for even one failure. In an eventually synchronous system, however, we can solve consensus only if there are &lt; n/2 crash failures, where n is the number of servers. If there are &#x2265; n/2 crash failures, consensus is again impossible. (In this case, a partitioning argument, much as in the CAP Theorem, leads to the impossibility.) Most practical consensus implementations today are designed (either explicitly or implicitly) for eventually synchronous systems [6, 8, 20].</p>
<p>&#x5728;&#x6700;&#x7EC8;&#x540C;&#x6B65;&#x7CFB;&#x7EDF;&#x4E2D;&#xFF0C;&#x53EA;&#x8981;&#x5B95;&#x673A;&#x7684;&#x670D;&#x52A1;&#x5C0F;&#x4E8E;&#x4E00;&#x534A;&#xFF0C;&#x5C31;&#x80FD;&#x8FBE;&#x6210;&#x4E00;&#x81F4;&#xFF1B;&#x5426;&#x5219;&#xFF0C;&#x5C31;&#x4E0D;&#x53EF;&#x80FD;&#x3002;</p>
<p><strong><em>Failure detectors</em></strong>. Another line of research has pursued a different approach to answering the question of how much synchrony is needed to solve consensus. Chandra et al. [7] introduced the idea of a failure detector, an oracle that provides sufficient information for processes to solve consensus in an asynchronous crash-prone system. In many ways, a failure detector exactly encapsulates the synchrony requirements for consensus. They showed [9] that a particular failure detector &#x2126; is the weakest failure detector for solving consensus. The failure detector &#x2126; essentially encapsulates a leader election service, and in fact, most practical consensus protocols today use such a service as an important component of their system&#x2014;the leader is often referred to as the &#x201C;master&#x201D; or the &#x201C;primary.&#x201D;</p>
<p><strong><em>Explicit assumptions</em></strong>. Finally, Aguilera et al. [1, 2] have explored a specific minimal set of link reliability and
synchrony assumptions sufficient for solving consensus. The protocols developed in these papers establish quite
minimal conditions for solving consensus.</p>
<h3 id="332-consistency">3.3.2 Consistency</h3>
<p>Another response to the impossibility of agreement has been an attempt to answer the question: what is the strongest form of consistency we can guarantee in a system with f crash failures?</p>
<p>&#x95EE;&#x9898;&#xFF1A;&#x5728;F&#x4E2A;&#x670D;&#x52A1;&#x53EF;&#x80FD;&#x5B95;&#x673A;&#x7684;&#x5206;&#x5E03;&#x5F0F;&#x7CFB;&#x7EDF;&#xFF0C;&#x80FD;&#x8FBE;&#x5230;&#x591A;&#x5F3A;&#x7684;&#x4E00;&#x81F4;&#x6027;&#xFF1F;</p>
<p>In an attempt to answer this question, Chaudhuri [10] introduced the problem of set agreement. Much like in consensus, each process begins with some value and eventually chooses an output. (That is, the validity and termination conditions are identical.) Unlike consensus, however, some disagreement in the output is allowed. Specifically, for the problem of k-set agreement, there may be up to k-different output values.</p>
<p>This weaker consistency guarantee leads to a sequence of problems: 1-set agreement, 2-set agreement, 3-set agreement, . . ., n-set agreement. Notice that 1-set agreement is identical to consensus, and n-set agreement is trivial (i.e., each process simply outputs its own initial value). Thus we know that 1-set agreement is impossible if there is even one crash failure, and n-set agreement can tolerate an arbitrary number of crash failures.</p>
<p>&#x6269;&#x5C55;&#x4E86;&#x4E00;&#x81F4;&#x6027;&#x534F;&#x8BAE;&#x3002;</p>
<p>In a seminal sequence of papers (a subset of which were awarded the Godel Prize in 2004) Borowski, Gafni, Herlihy, Saks, Shavit, and Zaharoglou [4, 17, 23] showed that k-set agreement can be solved if and only if there are at most k &#x2212; 1 crash failures. (At the same time, they revealed a deep connection between distributed computing and algebraic topology). Thus, in a sense, k-set agreement is the &#x201C;most&#x201D; agreement you can get in a system with k &#x2212; 1 failures, if you want to ensure availability.</p>
<p>&#x5728;K&#x4E2A;&#x670D;&#x52A1;&#x53EF;&#x80FD;&#x5B95;&#x673A;&#x7684;&#x5206;&#x5E03;&#x5F0F;&#x7CFB;&#x7EDF;&#xFF0C;&#x80FD;&#x8FBE;&#x5230; K+1 &#x96C6;&#x5408;&#x7684;&#x4E00;&#x81F4;&#x6027;&#xFF08;&#x5141;&#x8BB8;&#x8FD4;&#x56DE;K+1&#x4E2A;&#x4E0D;&#x540C;&#x7684;&#x503C;&#xFF09;&#x3002;</p>
<p>Chaudhuri et al. [11] further developed the techniques of [17], relating the degree of consistency to the running time of k-set agreement: in a synchronous system with t failures, at least t/k +1 rounds are necessary and sufficient.</p>
<p>From a theoretical perspective, these results for k-set agreement relate the strength of the consistency requirement to the availability of the system.</p>
<h1 id="4-practical-implications">4 Practical Implications</h1>
<p>Despite the negative implications of the CAP Theorem, practitioners building distributed services must still do the impossible. And in fact, they have continued to successfully build and deploy systems, addressing and overcoming the challenges posed by the CAP Theorem in various ways.</p>
<p>When dealing with unreliable networks, there are seemingly only two reasonable approaches: sacrifice availability or sacrifice consistency. This is the implication of the CAP Theorem: we cannot achieve consistency and availability in a partition-prone network. However, there is also another interesting approach that shows up frequently in practice: a larger system is segmented into different subsystems, each of which may choose a different trade-off. Moreover, this segmentation may take place along several different dimensions. From the perspective of the entire system, it may seem as if the software architects have sacrificed both consistency and availability! Yet the resulting design often yields a system that both responds well to most user requests, even under bad network conditions, and also provides high levels of consistency where consistency is required.</p>
<p>In this section, we will give some examples of designs that weaken availability (Section 4.1) and consistency (Section 4.2), and then discuss several approaches that guarantee neither consistency nor availability (Sections 4.3and 4.4).</p>
<h2 id="41-best-effort-availability">4.1 Best Effort Availability</h2>
<p>Perhaps the most common approach to dealing with unreliable networks is to design a service that guarantees consistency,i.e., correct operation, regardless of the network behavior. The service is then optimized to provide best effor availability, i.e., to be as responsive as is possible given the current network conditions.</p>
<p>This design makes sense when operating in a network in which communication is typically reliable and timely, and it is only on rare occasions that partitions or other network anomalies occur. When all the servers running a service are located in the same data center, this approach is a good one.</p>
<p>&#x6700;&#x5927;&#x52AA;&#x529B;&#x53EF;&#x7528;&#xFF08;&#x9996;&#x5148;&#x4FDD;&#x6301;&#x4E00;&#x81F4;&#xFF09;&#xFF1A;&#x57FA;&#x4E8E;&#x7F51;&#x7EDC;&#x901A;&#x4FE1;&#x5927;&#x90E8;&#x5206;&#x65F6;&#x95F4;&#x662F;&#x53EF;&#x9760;&#x7684;&#xFF0C;&#x53EA;&#x6709;&#x5C11;&#x6570;&#x60C5;&#x51B5;&#x624D;&#x4F1A;&#x5F02;&#x5E38;&#x3002;&#x5F53;&#x6240;&#x6709;&#x7684;&#x670D;&#x52A1;&#x90FD;&#x662F;&#x57FA;&#x4E8E;&#x4E00;&#x4E2A;&#x6570;&#x636E;&#x4E2D;&#x5FC3;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x79CD;&#x7B56;&#x7565;&#x662F;&#x597D;&#x7684;&#x3002;&#x4F8B;&#x5982;&#x8C37;&#x6B4C;&#x7684;&#x5206;&#x5E03;&#x5F0F;&#x9501;&#x670D;&#x52A1;&#xFF08;Chubby Lock Service&#xFF09;&#x3002;</p>
<p>A recent popular example of this approach is the Chubby Lock Service [5, 8], which was built by Google and is used extensively in the Google infrastructure (e.g., it supports the Google File System, BigTable, etc.). The Chubby Service, essentially, provides a simple file system optimized for small files. It is used, primarily, to share metadata and to provide a centralized point to manage coarse-grained locks in a distrusted manner. (Google has reported that one of its primary uses, today, is as a naming service that replaces DNS.)</p>
<p>Chubby provides strong consistency: at its heart is a distributed database, based on a primary-backup design. Consistency among the servers is ensured by using a replicated state machine protocol (specifically, Paxos [20]) to maintain synchronized logs. Chubby continues to operate as long as no more than half the servers fail, and it is guaranteed to make progress whenever the network is reliable. If the Chubby servers were to be partitioned, the service would become unavailable.</p>
<p>On the other hand, Chubby is optimized for the case where there is a stable primary and there are no partitions. In this case, it delivers a very high degree of availability. Significant design and engineering efforts have gone into ensuring that under such circumstances, Chubby can respond to requests very quickly. (For example, &#x201C;read-only&#x201D; requests are prioritized, and the replicated state machine protocol is modified so that such requests do not need to incur any additional network traffic.)</p>
<p>This design works well for Chubby, as each Chubby &#x201C;cell&#x201D; is deployed in a single data center Communication within a Chubby cell is typically fast and reliable, and the failure of a primary is not too frequent. (Users of the Chubby service are still encouraged to expect, and not treat as an error, occasional delays due to changes in the primary.) Thus Chubby provides guaranteed consistency, and a very high level of availability in the common case.</p>
<h2 id="42-best-effort-consistency">4.2 Best Effort Consistency</h2>
<p>For some applications, sacrificing availability is not an option: users require that it be responsive in all situations. Moreover, when the application is deployed over a wide area (rather than within a data center), the level of availability that can be achieved by a strongly consistent service may degrade rapidly. In such situations, designers sacrifice consistency: a response (and preferably a fast response) is guaranteed at all times. As a result, the response may not always be correct. Consistency is provided only in a best effort sense.</p>
<p>&#x6700;&#x5927;&#x52AA;&#x529B;&#x4E00;&#x81F4;&#xFF08;&#x9996;&#x5148;&#x4FDD;&#x8BC1;&#x53EF;&#x7528;&#xFF09;&#xFF1A;&#x89C6;&#x9891;&#x7B49;&#x6D41;&#x5A92;&#x4F53;&#xFF0C;&#x7528;&#x6237;&#x5173;&#x5FC3;&#x80FD;&#x5426;&#x53EF;&#x4EE5;&#x89C2;&#x770B;&#xFF08;&#x53EF;&#x7528;&#xFF09;&#xFF0C;&#x5176;&#x6B21;&#x624D;&#x662F;&#x6211;&#x770B;&#x7684;&#x662F;&#x4E0D;&#x662F;&#x6700;&#x65B0;&#x7684;&#xFF08;&#x4E00;&#x81F4;&#xFF09;&#x3002;</p>
<p>The classic example of this is web caching, as pioneered by Akamai (and others). Web content (e.g., images and video) is cached on servers that are placed in data centers throughout the world. Whenever a user requests a given web page, the content can be delivered from a nearby web cache. Such a system guarantees a very high level of availability: the proximity of the cache servers to the end users ensure both that the responses are rapid, and also that network connectivity issues rarely prevent a response.</p>
<p>On the other hand, the consistency guarantees are (potentially) quite minimal. When a web page is updated, it may take some time for the new content to propagate to all the cache servers. There is no guarantee that all users accessing a web page at any given time receive the exact same content. The caching service does its best to provide up-to-date content.</p>
<p>For Akamai (and other content delivery networks), this trade-off makes sense. Users viewing content on the web do not necessarily require strong consistency (i.e., two different users in different locations may view two slightly different versions of a web page). If the content viewed by a user is slightly out-of-date, there is usually little harm.</p>
<p>On the other hand, users loading a web page have little patience: a fast response is critical. (The problem of fast response time is even more problematic on mobile wireless devices.) </p>
<p>In addition, users are geographically located around the world. (This can be contrasted with Chubby, which is primarily used within a single data center.) The geographic dispersal implies that to achieve sufficient availability (and performance), consistency must be sacrificed.</p>
<h2 id="43-trading-consistency-for-availability">4.3 Trading Consistency for Availability</h2>
<p>There has also been some effort to more precisely tune the trade-off between consistency and availability. For example,it may be acceptable for some content to be one hour out of date, but not one day out of date. As long as the network connectivity has been good recently (i.e., in the last hour), we should be able to provide this level of consistency with good availability. On the other hand, if there are long-lasting partitions (e.g., more than one day), than it will be impossible to remain available. By setting the threshold for how out of date the data can be, the system designer can recisely specify the CAP trade-off.</p>
<p>This idea was explored in some detail by Yu and Vahdat [24, 25]. They developed a theory of &#x201C;continuous consistency&#x201D; and implemented the TACT toolkit, which enables replicated applications to specify exactly the desired consistency (in terms of conits, a unit of continuous consistency).</p>
<p>&#x8FDE;&#x7EED;&#x7684;&#x4E00;&#x81F4;&#x6027;&#xFF1F;
&#x6BD4;&#x5982;&#x8BF4;&#x8BA2;&#x7968;&#x7CFB;&#x7EDF;&#xFF0C;&#x5728;&#x7968;&#x5145;&#x8DB3;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5148;&#x4FDD;&#x8BC1;&#x53EF;&#x7528;&#xFF1B;&#x5728;&#x7968;&#x4E0D;&#x8DB3;&#x9700;&#x8981;&#x62A2;&#x7968;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8981;&#x5148;&#x4FDD;&#x8BC1;&#x4E00;&#x81F4;&#x3002;</p>
<p>An interesting component of the TACT toolkit is that the desired level of consistency can be updated dynamically, as the application executes. An airline reservation system is one example (discussed in [24]) of why this might be useful. When most of the seats on the airplane are available, it is usually safe for the reservation system to rely on somewhat out-of-date data&#x2014;even if a few additional seats have been reserved, the new reservation can likely be accommodated. As the plane is filled, however, the reservation system requires increasingly accurate data to ensure that the plane is not overbooked. Using TACT, the reservation system could request increasing levels of consistency as the number of available seats diminishes.</p>
<p>Notice that such a system provides neither strong consistency nor guaranteed availability: data can be out of date (i.e,. inconsistent), and yet if there is a significant network partition, the service may still be unavailable. Even so, this type of trade-off often makes sense as it may significantly increase the level of network disruption that the service can tolerate before it must compromise consistency or availability.</p>
<h2 id="44-segmenting-consistency-and-availability">4.4 Segmenting Consistency and Availability</h2>
<p>Many systems do not have a single uniform requirement. Some aspects of the system require strong consistency, and some require high availability. A quite natural approach to the limitations of the CAP Theorem is to redesign the system, segmenting it into components that provide different types of guarantees. In doing so, we again may end up with a service that, as a whole, guarantees neither consistency nor availability. Yet in the end, each part of the service provides exactly what is needed.</p>
<p>&#x4E00;&#x4E2A;&#x7CFB;&#x7EDF;&#x5404;&#x4E2A;&#x6A21;&#x5757;&#x7684;&#x9700;&#x6C42;&#x4E0D;&#x4E00;&#x81F4;&#xFF0C;&#x90A3;&#x5C31;&#x6309;&#x9700;&#x63D0;&#x4F9B;&#x53EF;&#x7528;&#x6027;&#x4E0E;&#x4E00;&#x81F4;&#x6027;&#x3002;</p>
<p>In this section, we discuss some of the dimensions along which a system might be partitioned. It is not always clear the precise guarantees that such segmentation provides, as it tends to be specific to the given application and the particular partitioning. It remains an open question to better understand these types of partitioning schemes.</p>
<p>&#x5206;&#x6BB5;&#x4E00;&#x81F4;&#x6027;&#x548C;&#x53EF;&#x7528;&#x6027;&#x3002;</p>
<p>&#x4EE5;&#x4E0B;&#x8BA8;&#x8BBA;&#x5206;&#x79BB;&#x7684;&#x60C5;&#x51B5;&#xFF1A;</p>
<p><strong><em>Data partitioning</em></strong>. Different types of data may require different levels of consistency and availability. For example, an on-line shopping cart may be highly available, responding rapidly to user requests; yet it may be occasionally inconsistent, losing a recent update in anomalous circumstances. The on-line product information for an e-commerce site may be somewhat inconsistent: users will tolerate somewhat out-of-date inventory information. The check-out/billing/shipping records, however, have to be strongly consistent: a user will be very unhappy if a finalized order does not reflect her intended purchase. When designing a system, different data may require different trade-offs.</p>
<p>&#x6570;&#x636E;&#x5206;&#x79BB;&#xFF0C;&#x65E5;&#x5FD7;&#x4FDD;&#x8BC1;&#x4E00;&#x81F4;&#xFF0C;DNS&#x670D;&#x52A1;&#x4FDD;&#x8BC1;&#x53EF;&#x7528;&#x3002;</p>
<p><strong><em>Operation partitioning</em></strong>. A second line along which an application can be partitioned is the operational dimension. Different operations may require different levels of consistency and availability. As a simple example, consider a system that guarantees high availability for read-only operations, while operations that modify the database may not respond during network partitions. Moreover, different types of updates might provide different levels of consistency:a &#x201C;purchase&#x201D; operation should guarantee consistency, while a &#x201C;query&#x201D; operation might return out of date data. The PNUTS system [12], implemented by Yahoo, provides exactly this style of guarantee with differing trade-offs for different types of read and write operations. In order to achieve a good user experience, different operations may require different trade-offs.</p>
<p>&#x64CD;&#x4F5C;&#x5206;&#x79BB;&#xFF0C;&#x8BFB;&#x64CD;&#x4F5C;&#x53EF;&#x7528;&#xFF0C;&#x5199;&#x64CD;&#x4F5C;&#x4E00;&#x81F4;&#x3002;</p>
<p><strong><em>Functional partitioning</em></strong>. Many services can be divided into different subservices which have different requirements.For example, an application might use a service such as Chubby (discussed in Section 4.1) for coarse-grained locks and distributed coordination (i.e., strong consistency). It might at the same time use a service such as DNS to handle naming: DNS is a classic example of caching, providing relatively weak consistency but high availability. The same service might use yet a third subservice, with a different consistency/availability trade-off for content distribution.</p>
<p>&#x7528;&#x6237;&#x5206;&#x79BB;</p>
<p><strong><em>User partitioning</em></strong>. Network partitions, and poor network performance in general, typically correlate with real geographic distance: users that are far away are more likely to see poor performance. Thus a service like Craigslist might elect to divide its servers among two different data centers&#x2014;one on the east coast of the US and one on the west coast of the US. Craigslist users from cities in California rely on the west coast data center, and hence get high availability.Moreover, since the California data is stored and maintained within a single west coast data center, consistency among the west coast servers can be readily achieved. The same holds true for east coast users that rely on the east coast datacenter. (On the other hand, users from New York inquiring about Craigslist ads in San Francisco may see less good performance, under this design.) Similarly, one could imagine that a social networking site might try to partition its users, ensuring high availability among groups of friends. (Again, the geographic correlation of friendships makes this
more feasible.)</p>
<p>&#x5C42;&#x6B21;&#x5206;&#x79BB;&#xFF0C;&#x6BD4;&#x5982;&#x5730;&#x56FE;&#xFF0C;&#x5728;&#x672C;&#x5E02;&#x80AF;&#x5B9A;&#x5148;&#x4FDD;&#x8BC1;&#x4E00;&#x81F4;&#xFF0C;&#x672C;&#x56FD;&#x4FDD;&#x8BC1;&#x53EF;&#x7528;&#x5C31;&#x884C;&#x4E86;&#x3002;</p>
<p><strong><em>Hierarchical partitioning</em></strong>. Some applications are organized hierarchically, partitioning along these different dimensions multiple times. At the top level, an application encompasses the entire world or the entire database; subsequent levels of the hierarchy partition the world into geographically smaller pieces, or the database into smaller parts. At each level of the hierarchy, the system may provide a different level of performance: better availability toward the leaves, or less consistency toward the root. For example, as you descend a geographically-organized hierarchy, the limitations of the CAP Theorem becomes less and less onerous as the relevant servers become better and better connected.</p>
<h1 id="5-the-cap-theorem-in-future-systems">5 The CAP Theorem in Future Systems</h1>
<p>As we have discussed in this paper, the CAP Theorem is one example of a fundamental trade-off between safety and liveness in fault-prone systems. Examining this inherent trade-off yields some insights into how systems can be designed to meet an application&#x2019;s needs, despite unreliable networks: software architects have explored strongly consistent solutions, with best-effort availability; they have explored weakly consistent solutions with high availability; and they have explored systems that mix both weaker availability and weaker consistency in varying ways.</p>
<p>At the same time, the networked world has changed significantly in the last ten years, creating new challenges for system designers, and new areas in which these same inherent trade-offs can be explored. We need new theoretical insights to address these challenges, and new techniques for coping with the problem in real-world systems.</p>
<p><strong><em>Scalability</em></strong>: Increasingly, we require that our systems be scalable, designed not just for today&#x2019;s customers but also for growth tomorrow. Intuitively, we think of a system as scalable if it can grow efficiently, using new resources efficiently to handle more load. There appear to be inherent trade-offs between scalability and consistency. For example, in order to efficiently use new resources, there must be coordination among those resources; the consistency required for this coordination appears subject to the CAP Theorem trade-offs. Studying this question may help to explain why even within a data center, where there are rarely partitions, it seems difficult to efficiently scale strongly consistent protocols(like Paxos [20]).</p>
<p><strong><em>Tolerating attacks</em></strong>: The CAP Theorem focuses on network partitions: sometimes, some servers cannot communicate reliably. Increasingly, however, we are seeing more severe attacks on networks. For example, denial-of-service attacks are becoming a near continuous threat to everyday network operations. A denial-of-service attack, however,cannot simply be modeled as a network partition. Similarly, we are seeing problems with malicious users hacking servers and otherwise disrupting major internet services. Tolerating these more problematic forms of disruption requires a somewhat different understanding of the fundamental consistency/availability trade-offs.</p>
<p><strong><em>Mobile wireless networks</em></strong>: The CAP Theorem initially focused on wide-area internet services. Today, however, a significant (and increasing) percentage of internet traffic is initiated by mobile devices. Many of the same trade-offs explored in the context of the CAP Theorem also hold in mobile networks&#x2014;and many of the problems are even harderto resolve.</p>
<p>Notably, wireless communication is notoriously unreliable. The key problem that motivated the CAP Theorem was the frequency of semi-stable partitions that change every few minutes. In a wireless networks, partitions are less common. However, unpredictable message loss is very common, and message latencies can vary significantly.</p>
<p>In addition, the types of applications being deployed in wireless networks may be somewhat different. The CAP Theorem was motivated by internet search engines and e-commerce web sites. There is a new generation of wireless applications, however, that tend to focus on different priorities: geography and proximity are critical; social interactions are primary; and privacy has a somewhat more immediate meaning. For example, consider foursquare, an application in which users check-in to locations, and initiate comments and discussion based on where they are.</p>
<p>&#x597D;&#x597D;&#x4F7F;&#x7528;CAP&#x7406;&#x8BBA;&#xFF0C;&#x56E0;&#x5730;&#x5236;&#x5B9C;&#x3002;</p>
<p>By re-examining the CAP Theorem in the context of wireless networks, we may hope to better understand the unique trade-offs that occur in these types of scenarios.</p>
<h1 id="references">References</h1>
<p>[1] Marcos Kawazoe Aguilera, Carole Delporte-Gallet, Hugues Fauconnier, and Sam Toueg. Communicationefficient
leader election and consensus with limited link synchrony. In The Proceedings of the International
Symposium on Principles of Distributed Computing (PODC), pages 328&#x2013;337, 2004.</p>
<p>[2] Marcos Kawazoe Aguilera, Carole Delporte-Gallet, Hugues Fauconnier, and Sam Toueg. On implementing
omega in systems with weak reliability and synchrony assumptions. Distributed Computing, 21(4):285&#x2013;314,
2008.</p>
<p>[3] Dan Alistarh, Seth Gilbert, Rachid Guerraoui, and Corentin Travers. How to solve consensus in the smallest
window of synchrony. In The Proceedings of the International Symposium on Distributed Computing (DISC),
pages 32&#x2013;46, 2008.</p>
<p>[4] E. Borowsky and E. Gafni. Generalized FLP impossibility result for t-resilient asynchronous computations. In
The Proceedings of the Symposium on Theory of Computing (STOC), pages 91&#x2013;100, 1993.</p>
<p>[5] Michael Burrows. The chubby lock service for loosely-coupled distributed systems. In The Proceedings of the
Symposium on Operating System Design and Implementation (OSDI), pages 335&#x2013;350, 2006.</p>
<p>[6] Miguel Castro and Barbara Liskov. Practical byzantine fault tolerance and proactive recovery. ACM Transactions
on Computer Systems, 20(4):398&#x2013;461, 2002.</p>
<p>[7] T. D. Chandra and S. Toueg. Unreliable failure detectors for reliable distributed systems. Journal of the ACM,
43(2):225&#x2013;267, 1996.</p>
<p>[8] Tushar D. Chandra, Robert Griesemer, and Joshua Redstone. Paxos made live: an engineering perspective. In The
Proceedings of the International Symposium on Principles of Distributed Computing (PODC), pages 398&#x2013;407,
New York, NY, USA, 2007.</p>
<p>[9] Tushar Deepak Chandra, Vassos Hadzilacos, and Sam Toueg. The weakest failure detector for solving consensus.
Journal of the ACM, 43(4):685&#x2013;722, 1996.</p>
<p>[10] S. Chaudhuri. More choices allow more faults: Set consensus problems in totally asynchronous systems. Information
&amp; Computation, 105(1):132&#x2013;158, 1993.</p>
<p>[11] S. Chaudhuri, M. Herlihy, N. A. Lynch, and M. R. Tuttle. A tight lower bound for k-set agreement. In The
Proceedings of the Symposium on Foundations of Computer Science (FOCS), pages 206&#x2013;215. IEEE, 1993.</p>
<p>[12] Brian F. Cooper, Raghu Ramakrishnan, Utkarsh Srivastava, Adam Silberstein, Philip Bohannon, Hans-Arno
Jacobsen, Nick Puz, Daniel Weaver, and Ramana Yerneni. Pnuts: Yahoo!&#x2019;s hosted data serving platform. PVLDB,
1(2):1277&#x2013;1288, 2008.</p>
<p>[13] P. Dutta and R. Guerraoui. The inherent price of indulgence. In The Proceedings of the International Symposium
on Principles of Distributed Computing (PODC), pages 88&#x2013;97, 2002.</p>
<p>[14] C. Dwork, N. Lynch, and L. Stockmeyer. Consensus in the presence of partial synchrony. Journal of the ACM,
35(2):288&#x2013;323, 1988.</p>
<p>[15] M. J. Fischer, N. A. Lynch, and M. S. Paterson. Impossibility of distributed consensus with one faulty process.
Journal of the ACM, 32(2):374&#x2013;382, 1985.</p>
<p>[16] Seth Gilbert and Nancy Lynch. Brewer&#x2019;s conjecture and the feasibility of consistent, available, partition-tolerant
web services. SigAct News, June 2002.</p>
<p>[17] M. Herlihy and N. Shavit. The topological structure of asynchronous computability. Journal of the ACM,
46(6):858&#x2013;923, 1999.</p>
<p>[18] L. Lamport and M. Fischer. Byzantine generals and transaction commit protocols. Unpublished, April 1982.</p>
<p>[19] Leslie Lamport. Time, clocks, and the ordering of events in a distributed system. Communications of the ACM,
21(7):558&#x2013;565, 1978.</p>
<p>[20] Leslie Lamport. The part-time parliament. ACM Transactions on Computer Systems, 16(2):133&#x2013;169, 1998.</p>
<p>[21] Butler W. Lampson. How to build a highly available system using consensus. In Workship on Distributed
Algorithms (WDAG), pages 1&#x2013;17, London, UK, 1996. Springer-Verlag.</p>
<p>[22] Nancy A. Lynch. Distributed Algorithms. Morgan Kaufman, 1996.</p>
<p>[23] M. E. Saks and F. Zaharoglou. Wait-free k-set agreement is impossible: The topology of public knowledge.
SIAM Journal of Computing, 29(5):1449&#x2013;1483, 2000.</p>
<p>[24] Haifeng Yu and Amin Vahdat. Design and evaluation of a conit-based continuous consistency model for replicated
services. ACM Transactions on Computer Systems, 20(3):239&#x2013;282, 2002.</p>
<p>[25] Haifeng Yu and Amin Vahdat. The costs and limits of availability for replicated services. ACM Transactions on
Computer Systems, 24(1):70&#x2013;113, 2006.</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../" class="navigation navigation-prev " aria-label="Previous page: 1 Algorithm">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="ZAB协议.html" class="navigation navigation-next " aria-label="Next page: 1.2 ZAB协议">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"1.1 CAP理论","level":"1.2.1","depth":2,"next":{"title":"1.2 ZAB协议","level":"1.2.2","depth":2,"path":"algorithm/ZAB协议.md","ref":"algorithm/ZAB协议.md","articles":[]},"previous":{"title":"1 Algorithm","level":"1.2","depth":1,"path":"README.md","ref":"README.md","articles":[{"title":"1.1 CAP理论","level":"1.2.1","depth":2,"path":"algorithm/CAP.md","ref":"algorithm/CAP.md","articles":[]},{"title":"1.2 ZAB协议","level":"1.2.2","depth":2,"path":"algorithm/ZAB协议.md","ref":"algorithm/ZAB协议.md","articles":[]},{"title":"1.3 分布式锁","level":"1.2.3","depth":2,"path":"algorithm/分布式锁.md","ref":"algorithm/分布式锁.md","articles":[]},{"title":"1.4 RAFT算法","level":"1.2.4","depth":2,"path":"algorithm/raft-zh_cn.md","ref":"algorithm/raft-zh_cn.md","articles":[]},{"title":"1.5 3分钟理解PAXOS算法","level":"1.2.5","depth":2,"path":"algorithm/PAXOS.md","ref":"algorithm/PAXOS.md","articles":[]}]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["livereload"],"pluginsConfig":{"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"algorithm/CAP.md","mtime":"2018-01-22T03:16:10.213Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2018-06-26T13:49:56.226Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

