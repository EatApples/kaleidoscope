# 一，服务端

### 1. 服务端之间如何通信的

1.X 版本使用 HTTP 协议，2.X 版本使用 GRPC 协议。

### 2. 服务端之间如何感知彼此

重点来了，每个 nacos 服务器是怎么保证 healthyList（server 列表）是一样的呢？

解决的办法仍然是心跳。

DistroMapper 初始化的时候会启动一个 ServerStatusReporter

ServerStatusReporter 会向其他的 nacos 服务器发送心跳，证明自己是健康的。

ServerStatusReporter 的 run 方法的 finally 会继续调用自己，这样就相当于是个定时任务了。

如果一直没收到其他 nacos 服务器的心跳信息呢，这里就有个神奇的逻辑了：ServerStatusReporter 是会模拟发送心跳给自己，保证 healthyList 的逻辑一定会执行。

### 3. 服务端如何保证高可用

distro 协议的工作流程如下：

（1）Nacos 启动时首先从其他远程节点同步全部数据。
（2）Nacos 每个节点是平等的都可以处理写入请求，同时把新数据同步到其他节点。
（3）每个节点只负责部分数据，定时发送自己负责数据的校验值到其他节点来保持数据一致性。

读取操作则不需要路由，因为集群中的各个节点会同步服务状态，每个节点都会有一份最新的服务数据。

而当节点发生宕机后，原本该节点负责的一部分服务的写入任务会转移到其他节点，从而保证 Nacos 集群整体的可用性。

一个比较复杂的情况是，节点没有宕机，但是出现了网络分区。

这个情况会损害可用性，客户端会表现为有时候服务存在有时候服务不存在。

综上，Nacos 的 distro 一致性协议可以保证在大多数情况下，集群中的机器宕机后依旧不损害整体的可用性。该可用性保证存在于 nacos-server 端。

### 4. 服务端之间如何保持数据一致性

那要如何保证数据的最终一致呢？其实是通过心跳来保证的。

leader 发送心跳的时候会带上所有的 key 和 key 对应的时间戳，follower 发现 key 不存在或者时间戳比较小，就发送请求给 leader 拿这些 key 的数据，最终达到数据一致。

这些数据会有点大，所以做了 gzip 压缩。

follower 收到心跳的时候，判断 key 的存在和时间戳，和 leader 对不上的，发起请求拿最新的。

不一致的 key 可能比较多，所以批量去拿，每次拿 50 个 key。

数据差距可能比较大，一直循环拿的话对 leader 的压力会比较大，所以拿一次之后 sleep 200ms。

HttpClient 异步去获取这些内容，异步回来可能是不同的线程，会有多线程的问题，所以要加锁。

当然这里 HttpClient 是同步请求，也会是要加锁的，因为收到心跳请求是不同的线程，如果心跳处理比较慢，也变成多线程处理了。

除了 key 的新增和更新，那 key 的删除怎么同步呢？

本地的所有的 key 放到一个 map，处理过的 key 标记一下，如果到最后还有 key 没标记，说明是本地有，但是 leader 没有。这种 key 就是 leader 已经删除的 key 了。

### 5. Nacos 中使用了 UDP 吗？作用是什么？

变化的通知 push 采用的是 udp。

### 6. Nacos 中怎么判断实例是否过期，是只判断自己负责的那部分吗？

如果重复的做，会有很大的资源浪费，而且如果都检查到超时了，都和 leader 通信说要下线，对网络的负担也比较高。

解决的办法就是 服务名 hash % nacos 服务器数目 ，得到其中一台 nacos 服务器，如果是自己的话，就开始检查这个服务的实例列表，如果不是就跳过。

每个 nacos 服务器都这样去检查，自然会覆盖到所有服务的检查。

### 7. 服务端如何做心跳检查

方式一 tcp
方式二 http
方式三 mysql

### 8. 服务端如何检查持久节点的可用性？

服务端访问客户端的注册 IP 与 PORT。

### 9. 关于集群元数据的疑问

（1）raftPort 为 7848 在文档中并没有给出
（2）元数据中有 4 个 Raft 相关服务，分别有啥作用
naming_instance_metadata
naming_service_metadata
naming_persistent_service
naming_persistent_service_v2

# 二，客户端

### 1. 客户端如何获取服务端列表

主要是启动了一个线程，每隔 30s，去执行 refreshSrvIfNeed()这个方法。

refreshSrvIfNeed()这个方法里面，做的事情，是通过一个 http 请求，去 Nacos server 获取一串 Nacos server 集群的地址列表。

获取完地址列表后，赋值给 serversFromEndpoint，并且记录当前更新时间，在下一次更新时，小于 30s，就不更新，避免频繁更新，总的来说，NameProxy 的目的就是定时在客户端维护 Nacos 服务端的最新地址列表。

### 2. 客户端如何保证高可用

首先判定下容灾开关是否有，容灾开关是一个磁盘文件的形式存在，通过容灾开关文件名字，判定容灾开关是否打开，1 表示打开，0 为关闭，读取到容灾开关后，将值更新到内存中，后续解析地址列表时，首先会判定一下容灾开关是否打开，如果打开了，就读缓存的数据，否则从服务端获取最新数据。

概述下客户端初始化流程做的几件事：

（1）初始化事件分发组件，用于处理服务端主动通知下来的变更数据
（2）初始化 Nacos 服务集群地址列表更新组件，用于客户端维护 Nacos 服务端的最新地址列表
（3）初始化服务健康检查模块，主动给服务端上报服务的健康情况
（4）初始化客户端的缓存，10s 检查一次，如果没有，则创建
（5）24 小时备份一次客户端的缓存文件
（6）5s 检查一次容灾开关，更新到内存中，容灾模式情况下，服务地址读的都是缓存

经常有人说过，Nacos 有个好处，就是当一个服务挂了之后，短时间内不会造成影响，因为有个本地注册列表，在服务不更新的情况下，服务还能够正常的运转，其原因如下：

（1）Nacos 的服务发现，一般是通过订阅的形式来获取服务数据。
（2）而通过订阅的方式，则是从本地的服务注册列表中获取（可以理解为缓存）。相反，如果不订阅，那么服务的信息将会从 Nacos 服务端获取，这时候就需要对应的服务是健康的。（宕机就不能使用了）
（3）在代码设计上，通过 Map 来存放实例数据，key 为实例名称，value 为实例的相关信息数据（ServiceInfo 对象）。
