```
中文名：《操作系统导论》
英文名：《Operating Systems: Three Easy Pieces》
作者：Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau
英文版是免费开放的：http://pages.cs.wisc.edu/~remzi/OSTEP/
```

# 目录

不闻不若闻之，闻之不若见之，见之不若知之，知之不若行之——荀子

### 1. 操作系统为什么将资源虚拟化？

答案显而易见：让系统更易于使用。

操作系统将物理资源（CPU，磁盘，内存等）转换为更通用、更强大且更易于使用的虚拟形式。我们有时将操作系统称为虚拟机。

为了让用户使用虚拟机的功能，操作系统会提供系统调用。

操作系统有时被称为资源管理器，因为虚拟化让许多程序运行（从而共享 CPU），让许多程序可以同时访问各自的指令和数据（从而共享内存），让许多程序访问设备（从而共享磁盘等）

### 2. 操作系统实际上做了什么？

操作系统

（1）取得 CPU、内存、磁盘等物理资源，并对它们进行虚拟化。

（2）处理与并发有关的麻烦且棘手的问题。

（3）持久地存储文件，从而使它们长期安全。

这就是本书所讲的三个模块。

### 3. 操作系统设计目标

（1）抽象：建立一些抽象，让系统方便和易于使用（不用考虑底层）

（2）高性能：提供高性能，最小化操作系统的开销

（3）隔离/保护：在应用程序之间，应用程序与操作系统之间提供保护（也就是所谓的隔离）

（4）可靠性：应用程序都强依赖操作系统，操作系统必须不间断地运行，操作系统力求提供高度的可靠性

（5）高效，安全，可移植

### 4. 什么是系统调用？

系统调用将控制转移（跳转）到操作系统中，同时提高硬件特权级别。

用户应用程序以所谓的用户模式运行，这意味着硬件限制了应用程序的功能。例如，以用户模式运行的应用程序通常不能发起对磁盘的 IO 请求，不能访问任何物理内存页，或在网络上发送数据包。

在发起系统调用时（通常通过一个称为 trap 的特殊硬件指令），硬件将控制转移到预先指定的 trap 处理程序（即预先设置的操作系统），并同时将特权级别提升至内核模式。

在内核模式下，操作系统可以完全访问系统的硬件，因此可以执行诸如发起 IO 请求或为程序提供更多内存等功能。当操作系统完成请求的服务时，它通过特殊的 trap 返回指令，将控制权交还给用户。该指令（return-from-trap）返回到用户模式，同时将控制权交还给应用程序，回到应用离开的地方。

# 第一部分：虚拟化

### 1. 进程

进程——操作系统提供的基本抽象。进程的非正式定义言简意赅：进程就是运行中的程序。

进程可以处于以下 3 种状态之一：

- 运行：在 CPU 上运行，执行指令
- 就绪：已准备好运行，但未分配 CPU 时间片
- 阻塞：等待发生其他事件才会准备运行（就绪）

### 2. 如何提供有许多 CPU 的假象？

操作系统通过虚拟化 CPU 来提供这种假象。

通过让一个进程只运行一个时间片，然后切换到其他进程，即所谓的时分共享 CPU 技术。潜在的开销就是性能损失，因为每个进程的运行就会慢一点。

### 3. 机制和策略

机制：一些低级方法或协议，实现了所需的功能。例如上下文切换，它让操作系统能够停止运行一个程序，并开始在给定的 CPU 上运行另外一个程序。

策略：在这些机制之上，策略是在操作系统内做出某种决定的算法。例如给定一组可能的程序要在 CPU 上运行，操作系统如何抉择？操作系统的调度策略会做这样的决定。

### 4. 机制：受限直接执行

在保持控制权的同时，获得高性能，是构建操作系统的主要挑战之一。

硬件通过提供不同的执行模式来协助操作系统：用户模式和内核模式。

要执行系统调用，程序必须执行特殊的陷阱（trap）指令。问题是，陷阱如何知道在 OS 内执行哪些代码？

内核通过在启动时设置陷阱表来实现。内核必须谨慎地控制在陷阱上执行的代码。

操作系统做的第一件事，就是告诉硬件在发生某些异常事件时要运行哪些代码。

### 5. 进程切换时，操作系统如何重获 CPU 的控制权？

（1）协作方式：等待系统调用。大多数进程通过系统调用，将 CPU 的控制权转移给操作系统。如果应用程序执行了某些非法操作，也会将控制转移给操作系统。

（2）非协作方式：操作系统进行控制。需要硬件参与：时钟中断。启动时，操作系统必须启动时间，也必须通知硬件哪些代码在发生时钟中断时运行。

### 6. 上下文切换

操作系统为当前执行的进程保留一些寄存器的值（例如，保存到内核栈），并为即将执行的进程恢复一些寄存器的值（从内核栈读取）。

上下文切换的成本不仅仅来自寄存器的保存与恢复，程序运行时在 CPU 高速缓存，TLB，分支预测器和其他硬件中建立了大量的状态，切换时会导致状态的刷新，这可能导致显著的性能成本。

### 7. 进程调度

调度指标：周转时间=完成时间-到达时间。

调度策略：

（1）先进先出（FIFO）。可能带来护航效应，即一些耗时较少的潜在资源消费者，被排在重量级的资源消费者之后，导致周转时间变长。

（2）最短任务优先（SJF）。先运行最短的任务，然后是次短的任务，如此下去。也可能有护航问题。

（3）最短完成时间优先（STCF），或抢占式最短作业优先（PSJF）。几乎所有现代化的调度程序都是抢占式的。向 SJF 添加抢占，可以解决护航问题。

调度指标：响应时间=首次运行-到达时间。

调度策略：

（1）轮转（RR）。RR 在一个时间片内运行一个任务，然后切换到运行队列中的下一个任务，而不是运行一个任务直到结束。RR 反复执行，直到所有任务都完成。系统设计者需要权衡时间片的长度，以便摊销上下文切换的成本，而又不会使系统不及时响应。

在周转时间与响应时间之间需要做权衡。第一种类型（SJF，STCF）优化周转时间，但对响应时间不利；第二种类型（RR）优化响应时间，但对周转时间不利。

### 8. 调度：多级反馈队列

多级反馈队列（MLFQ）需要解决 2 方面的问题：首先，优化周转时间；其次，降低响应时间。

多级反馈队列是用历史经验预测未来的典型例子，在操作系统中，使用此技术的有很多，比如硬件的分支预测及缓存算法。

多级反馈队列，它有多级队列，并利用反馈信息决定某个任务的优先级。以史为鉴：关注进程的一贯表现，然后区别对待。

MLFQ 的规则如下：

（1）先后规则：如果 A 的优先级> B 的优先级，运行 A（不运行 B）
（2）轮转规则：如果 A 的优先级=B 的优先级，轮转运行 A 和 B
（3）初始最高规则：任务进入系统时，放在最高优先级（最上层队列）
（4）用完下降规则：一旦任务用完其在某一层中的时间配额（不管中间主动放弃了多少次 CPU），就降低其优先级（移入低一级队列）
（5）不断重置规则：经过一段时间 S，就将系统中所有任务重新加入最高优先级队列

S 的值如何设置？如果 S 设置得太高，长任务会饥饿；如果设置得太低，交互型任务又得不到合适的 CPU 时间比例。

MLFQ 不需要对任务的运行方式有先验知识，而是通过观察工作的运行来给出对应的优先级。通过这种方式，使得短时间的任务能获得类似 SJF 的很好的全局性能，同时对长时间运行的 CPU 密集型负载也可以稳步推进。

### 9. 调度：比例份额

比例份额调度程序，也称为公平份额调度程序，调度程序的最终目标，是确保每个任务获得一定比例的 CPU 时间，而不是优化周转时间与响应时间。

比例份额有一个非常优秀的现代例子：彩票调度。越是应该频繁运行的进程，越是应该拥有更多地赢得彩票的机会。

彩票调度最精彩的地方在于利用了随机性。随机方法相对于传统的决策方式，至少有 3 点优势：

（1）避免奇怪的边界情况
（2）轻量，几乎不需记录任何状态
（3）很快

彩票调度中最不可思议的，或许就是实现简单。只需要一个不错的随机数生成器来选择中奖彩票和一个记录系统中所有进程的数据结构（一个列表），以及所有彩票的总数。

比如有 3 个任务 ABC，彩票总数为 60，它们获得的彩票数分别为：A-10，B-20，C-30。现使用链表将 ABC 排列如下：A-B-C（C-B-A 等都可以），表示 A 的区间为[1，10]，B 的区间为[11，30]，C 的区间为[31，60]。然后获取一个在 1~60 的随机数，比如 34，则 C 被调度（因为刚好落在 C 的区间段）。

如何为任务分配彩票？这是一个棘手的问题，没有最佳答案。

虽然随机方式可以使得调度程序的实现简单（且大致正确），但偶尔并不能产生正确的比例。步长调度解决了这个问题，它是一个确定性的公平分配方案。

步长调度也很简单。系统中每个人物都有自己的步长，这个值与票数值成反比。每次进程运行后，让它的计数器增加它的步长，记录总体的进展。之后，调度程序使用进程的步长及行程值来确定调度哪个进程。当需要进行调度时，选择目前拥有最小行程值的进程，并且在运行之后，将该进程的行程值增加一个步长。

比如以 60 为标准，前文 ABC 的步长分别为 A-6，B-3，C-2。因为刚开始调度时，ABC 的行程都是 0，随机选一个执行，比如 A，则 A 的行程为 6。之后 BC 中也随机选一个，比如选 B 执行，则 B 的行程为 3。之后肯定是 C 会执行，C 的行程为 2。此时，因为 C 的行程最短，则 C 会被再次执行。

在彩票调度中，票数值越高，理论上越容易被选中（基于大叔定律，概率趋于期望。但有例外，因为是随机性的）。而在步长调度中，票数值越高，步长越短，越容易被执行（小步快跑）。

彩票调度只能运行一段时间后，在概率上实现比例，而步长调度可以在每个调度周期后做到完全正确。既然如此，为什么还需要彩票调度？因为彩票调度有一个步长调度没有的优势：不需要全局状态。因此，彩票调度算法能够更合理地处理新加入的进程。

比例份额调度中，彩票调度通过随机值，聪明地做到了按比例分配。步长调度能够确定的获得需要的比例。但是，它们并没有作为 CPU 调度程序被广泛使用。一个原因是这两种方案都不能很好地适合 IO，另一个原因是其中最难的票数分配问题并没有确定的解决方案。

即使如此，在容易确定份额比例的场景（比如，在虚拟数据中心中，分配 1/4 的 CPU 周期给 Windows 系统，其他的给 Linux 系统），比例份额调度还能发挥优势：简单高效。

### 10. 多处理器调度

多处理器与单 CPU 的基本区别：区别的核心在于对硬件缓存的使用，以及多处理之间共享数据的方式。

（1）单队列多处理器调度（SQMS）。将所有需要调度的任务放在一个单独的队列中，然后按照 CPU 个数选择合适的任务来运行。

优势：简单，直接复用之前单 CPU 的调度方式。负载均衡好。

劣势：其一，缺乏可扩展性。多处理器环境下，需要加锁来保证正确性。而锁的使用非常耗时。其二，缺乏缓存亲和性。任务会在不同的处理器中执行，难以利用缓存的优势。

（2）多队列多处理器调度（MQMS）。基本调度框架包含多个调度队列，每个队列可以使用不同的调度策略。

优势：天生具有可扩展性。队列的数量会随着 CPU 的增加而增加，因此锁和缓存的争用都不是大问题。具有良好的缓存亲和度。

劣势：负载不均。如何解决？通过任务的跨 CPU 迁移，可以真正的实现负载均衡。系统如何发起这样的迁移？使用工作窃取（work stealing）技术。工作量较少的队列不定期地从其他较多的队列中“窃取”一个或多个工作，实现负载均衡。不定期到底是多长？找到合适的阈值仍然是黑魔法，这在系统策略设计中很常见。

（3）Linux 多处理器调度

在构建多处理器调度程序方面，Linux 社区一致没有达成共识。一直以来，存在 3 种不同的调度程序：

- O(1)调度程序，采用多队列，基于优先级的（类似 MLFQ），随着时间的推移改变进程的优先级，然后调度最高优先级进程，来实现调度目标。交互性得到了特别关注。
- 完全公平调度程序（CFS），采用多队列，基于确定的比例调度（类似步长调度）。
- BF 调度程序（BFS），采用单队列，基于比例调度，但采用更加复杂的方案，称为最早最合适虚拟截止时间优先算法（EEVEF）。

### 11. 虚拟内存的目标

地址空间，是操作系统提供的易用的物理内存抽象。

（1）透明：操作系统提供的假象不应该被应用程序看破，程序不应该感知到内存被虚拟化的事实。相反，程序的行为就好像它拥有自己的私有物理内存。

（2）效率：包括时间上（不会使程序运行得更慢）和空间上（不需要太多额外的内存来支持虚拟化）。在实现高效率虚拟化时，操作系统不得不依靠硬件支持，包括 TLB 这样的硬件功能。

（3）保护：操作系统应该确保进程受到保护，不会受其他进程影响，操作系统本身也不会受进程影响。保护在进程之间提供隔离的特性。

### 12. 内存泄漏

如果忘记释放内存，就会发生内存泄漏。

在长时间运行的应用程序或系统中，内存泄漏是一个巨大的问题。因为缓慢泄漏的内存会导致内存不足，此时需要重新启动。

一般来说，当你用完一段内存时，应当确保释放它。请注意，使用垃圾收集语言在这里没有什么帮助：如果你仍然持有某块内存的引用，那么垃圾收集器就不会释放它。

内存泄漏的根源在于人。

### 13. 机制：地址转换

基于硬件的地址转换，硬件对每次内存访问进行处理，将指令中的虚拟地址转换为数据实际存储的物理地址。

当然，仅仅依靠硬件不足以实现虚拟内存，操作系统必须在关键的位置介入，设置好硬件，以便完成正确的地址转换。

动态（基于硬件）重定位：具体来说，每个 CPU 需要 2 个硬件寄存器，基址寄存器和界限寄存器。进程中使用的内存引用都是虚拟地址，硬件接下来将虚拟机地址加上基址寄存器中的内容，得到物理地址，再发给内存系统。

有时候我们将 CPU 的这个负责地址转换的部分，称为内存管理单元（MMU）。

操作系统需要介入的关键时刻：
（1）内存管理：
需要为新进程分配内存；
从终止的进程回收内存；
一般通过空闲列表来管理内存；
（2）基址/界限管理
必须在上下文切换时，正确设置基址/界限寄存器；
（3）异常处理
当异常发生时执行的代码，可能的动作是终止犯错的进程；

通过虚拟化，操作系统（在硬件的帮助下）将丑陋的机器现实转化成一种有用的、强大的、易于使用的抽象。

### 14. 分段

内部碎片：已经分配的内存单元内部有未使用的空间（即碎片），造成了浪费。

简单的通过基址/界限寄存器实现的虚拟内存很浪费，因为栈和堆之前的空间并没有被进程使用，却依然占用了实际的物理内存。

为了解决这个问题，分段的概念应运而生。这个想法很简单：在 MMU 中引入不止一个基址/界限集群器对，而是给地址空间内的每个逻辑段一对。

一个段只是地址空间里的一个连续定长的区域，在典型的地址空间里有 3 个逻辑不同的段：代码、栈和堆。

系统运行时，地址空间中不同的段被重定位到物理内存中。与之前的整个地址空间只有一个基址/界限寄存器对的方式相比，节省了大量的物理内存（按需细粒度划分，消除碎片）。

外部碎片：空闲空间被分割成不同大小的小块，成为碎片，后续的请求可能失败，因为没有一块足够大的连续空闲空间，即使这时总的空闲空间超出请求的大小。

然而，分段也带来了新的问题：外部碎片（每个进程都有一些段，段的大小不同。物理内存很快充满了许多空闲空间的小洞，因为很难分配给新的段，或扩大已有的段。）。例如：一个进程需要分配一个 20KB 的段，当前有 24KB 的空闲，但并不连续（是不相邻的块）。因此，操作系统无法满足这 20 KB 的请求。

如何解决？

（1）紧凑物理内存，重新安排原有的段（类似 java 内存回收的复制算法）。

（2）一种更简单的做法是利用空闲列表管理算法，试图保留大的内存块用于分配。

### 15. 空闲空间管理

大多数分配程序采用的通用机制：
（1）分割与合并
（2）追踪已分配空间的大小
（3）嵌入空闲列表
（4）让堆增长

基本策略：
（1）最优匹配：遍历整个空闲列表，找到最接近请求大小的空闲块，尽量避免空间浪费（较高的性能代价）
（2）最差匹配：找到最大的空闲块，分配之后，将剩余的加入空闲列表（过量的碎片，很高的开销）
（3）首次匹配：找到第一个足够大的块，分配之后，将剩余的加入空闲列表（可能列表开头有很多小块）
（4）下次匹配：维护一个指针，指向上一次查找结束的位置（解决开头小块的问题）
（5）分离空闲列表（例如 SLAB，厚块分配程序）：分开管理。如果某个应用程序经常申请一种或几种大小的内存空间，那就用一个独立的列表，单独管理。其他大小的请求都交给更通用的内存分配程序
（6）伙伴系统（例如二分伙伴分配程序）：空闲空间从概念上被看成大小为 2^N 的大空间，当有一个内存分配请求时，空闲空间被递归地一分为二，直到刚好适配请求的大小（漂亮之处在于，块被释放时，很容易合并）

这些方法都有一个重要的问题：缺乏可扩展性。具体来说，就是查找列表会很慢。

因此，更先进的分配程序采用更复杂的数据结构来优化这个开销，牺牲简单性来换取性能。例子包括平衡二叉树、伸展树和偏序树等。

提示：如果有一千个解决方案，就没有特别好的

存在如此多的不同的算法来尝试减少外部碎片，正说明了解决这个问题没有最好的办法。因此我们满足于找到一个合理的足够好的方案。

唯一真正的解决方案就是，完全避免这个问题：永远不要分配不同大小的内存块。

### 16. 分页

操作系统有 2 种方法，来解决大多数空间管理的问题。

第一种是将空间分割成不同长度的分片，就像虚拟内存管理中的分段。然而空间本身会碎片化；

第二种是分页：将空间分割成固定长度的分片。

为了记录地址空间的每个虚拟页放在物理内存中的位置，操作系统通常为每个进程保存一个数据结构，即页表（存储虚拟-物理地址转换）。

分页有许多优点：

首先，分页不会导致外部碎片，因为分页将内存划分为固定大小的单元。
其次，分页非常灵活，支持稀疏虚拟地址空间。

然而，实现分页支持而不小心考虑，会导致系统变慢（有许多额外的内存访问来访问页表）和内存浪费（内存被页表塞满，而不是有用的数据）

### 17. 快速地址转换（TLB）

使用分页作为核心机制来实现虚拟内存，可能会带来较高的性能开销，因为需要访问页表将虚拟地址映射为物理地址。

如何加速地址转换？

我们需要增加所谓的地址转换旁路缓冲存储器（Translation-Lookaside Buffer，TLB），即频繁发生的虚拟到物理地址转换的硬件缓存（更好的名字应该是地址转换缓存）。对于每次内存访问，硬件先检查 TLB，看是否命中（有期望的转换映射），如果命中，就完成转换（很快），不用访问页表。

类似其他缓存，TLB 的成功依赖于空间和时间局部性。

谁来 TLB 未命中？可能有 2 个答案：硬件或软件（操作系统）

TLB 不能满足所有程序的需求，例如：上下文切换时需要对 TLB 进行处理。一个程序短时间内访问的页超过 TLB 的页数，就会产生大量未命中，运行速度就会变慢（超出 TLB 覆盖范围，一种解决方案是：支持更大的页）

### 18. 分页：较小的表

页表太大，因此消耗的内存太多。如何让页表更小？

简单的解决方案：更大的页。大页在数据库管理系统和其他高端商业应用程序中很常见。然而，大内存页会导致每页内的浪费，即内部碎片问题（因为浪费在分配单元内部）。因此，大多数系统在常见的情况下使用相对较小的页（4KB 或 8KB）。问题不会如此简单的解决。

混合方法：分页和分段。每当有 2 种合理但不同的方法时，你应该研究两者的结合，看能否两全其美（这种组合成为杂合，杂交混合）。我们杂合的方法是，不是为进程的整个地址空间提供单个页表，而是为每个逻辑分段提供一个（相对整个地址空间来说，段的空间显然很小，页表当然也小了）。但是，这种方案仍有问题：分段有外部碎片的可能。

多级页表：将线性页表变成类似树的东西。首先，将页表分成页大小的单元（页表项）。然后，对这些页表项建立页目录。页目录指向页表的各个部分，不要求页表是连续的了。多级表是一个时间-空间折中的例子（如果你希望更快地访问特定的数据结构，就必须为该结构付出空间的代价）。

多级页表的优势：
（1）紧凑，支持稀疏的地址空间。
（2）更容易管理内存
劣势：
（1）TLB 未命中时，需要加载 2 次（一次页目录，一次页表项），线性页表只需要 1 次
（2）为了节省宝贵的内存，页表查找更加复杂（通常我们愿意增加复杂性以提高性能或降低管理费用）

反向页表：页表世界中更极端的空间节省。只保留了一个页表，其中的项代表系统的每个物理页，而不是有许多页表（系统的每个进程都有一个）。页表项告诉我们哪个进程在使用此页，以及该进程的哪个虚拟页映射到此物理页（一般使用哈希表加速查找）。

当页表太大而无法一次性装入物理内存时，可以将页表中的一部分交换到磁盘。

### 19. 机制：交换空间

为什么我们要为进程支持巨大的地址空间？答案还是方便和易用性。

我们要做的第一件事情，就是在硬盘上开辟一部分空间用于物理页的移入和移出。在操作系统中，一般这样的空间称为交换空间。

如果希望允许页交换到硬盘，必须添加更多的机制。当硬件在页表项中查找时，可能发现页不在物理内存中。硬件可通过页表项的存在位判断。存在位为零，说明页不在内存中，而在磁盘上。

访问不在物理内存中的页时，会发生页错误（实际上，它应该被称为页未命中），操作系统被唤起来处理，一段称为“页错误处理程序”的代码会被执行。

当内存快满时，操作系统可能希望先交换出一个或多个页，以便为即将交换入的新页留出空间。选择哪些页被替换的过程，被称为页交换策略。

### 20. 替换策略

最优替换策略：理想中的，用作比较基准。

简单策略：FIFO。一般来说，当缓存变大时，缓存命中率是会提高的（变好）。但是采用 FIFO 策略，命中率可能反而下降（即 Belady 的异常）。

另一个简单策略：随机。随机取决于当时的运气。

利用历史数据：最不经常使用（LFU）：页替换策略可以使用的一个历史信息是频率。最近最少使用（LRU）：页更常用的属性是访问的近期性，越近被访问的过的页，也许再次访问的可能性也就越大。这些都是基于局部性原则。

由于实现完美的 LRU 代价非常昂贵，我们否实现一个近似的 LRU 算法，并且依然能够获得预期的效果？

答案是肯定的：从计算开销的角度来看，近似 LRU 更为可行。这个想法需要硬件增加一个使用位（有时称为引用位，用来记录历史使用信息）。有一个简单的方法称为时钟算法，周期性地检查使用位，替换使用位为 0 的页（表示最近未使用）。如果都被使用，则将所有页都置为 0。实际上，任何周期性地清除使用位，然后通过区分使用位是 1 和 0 来判定应该替换哪个页的方法都是可以的。

由于分页到硬盘非常昂贵，因此频繁分页的成本太高。所以，过度分页的最佳解决方案往往很简单：购买更多的内存！

# 第二部分：并发

# 第三部分：持久性

# 本书定律一览

### 1. Lampson 定律

重要的是做对事。抽象和简化都不能替代做对事。

有时你必须做正确的事，当你这样做时，总是好过其他方案。

### 2. Ousterhout 定律

尽可能避免巫毒常量。即避免设置魔法值。

巫毒常量，举例来说，比如多级反馈队列中，设置的优先级重置的时间 S。
